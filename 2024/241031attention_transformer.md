[블로그링크](https://velog.io/@gyu_p/Attention%EA%B3%BC-Transformer%EC%9D%98-%EC%9E%91%EB%8F%99-%EC%9B%90%EB%A6%AC)

[참고링크](https://www.youtube.com/watch?v=6s69XY025MU)
**유튜브 출처 Attention/Transformer 시각화로 설명(임커밋)**
유튜브 내용을 정리해봤다.

# Attention과 Transformer의 작동 원리


### 어텐션 설명의 필요성
 텐서는 정보를 나타내는 단위입니다. 텐서는 웨이트(weight) 매트릭스를 통해 정보를 계산하며, 이를 통해 다양한 정보를 조합합니다.

예시: 텐서를 두 개의 숫자로 나타낸다면, 각 숫자는 정보의 특정 속성을 나타내고, 웨이트를 통해 어느 속성이 더 중요한지를 계산하여 최종 값을 도출합니다.

첫 번째 성질 - 정보의 유지
어떤 정보를 표현하는 텐서는 웨이트를 통과해도 기존의 정보를 유지합니다. 이 특징은 텐서가 정보의 변형 없이 가중치(weight)를 통해 중요한 정보를 부각시키는 데 유용합니다.

예시: 만약 이미지에서 고양이를 인식하는 모델이라면, 고양이의 귀와 눈 같은 중요한 부분이 더 크게 반영됩니다. 웨이트를 거쳐도 고양이의 모양은 변하지 않지만, 중요한 부분이 강조됩니다.

### 어텐션의 기본 구조
어텐션의 성질 - 쿼리와 키의 관계
어텐션은 쿼리(Query)와 키(Key) 텐서의 관계를 통해 정보의 가중치를 계산합니다.

**쿼리 텐서**: 정보를 가진 텐서 하나입니다.
**키 텐서**: 서로 다른 정보를 가진 여러 개의 키 텐서들입니다.
쿼리와 키를 각각 웨이트에 통과시켜, 쿼리와 각 키 간의 유사성을 계산하고, 결과에 따라 정보를 가중치로 조합합니다.

예시: 검색 엔진에서 사용자가 입력한 쿼리와 유사한 웹페이지를 찾는 방식과 유사합니다.


### Weighted Sum의 정의와 역할
Weighted Sum은 여러 텐서를 가중치를 통해 조합하는 방식으로, 웨이트가 큰 정보가 더 많이 반영됩니다. 웨이트의 총합이 1이기 때문에, 특정 정보의 가중치가 커지면 다른 정보의 비율은 작아지게 됩니다.

예시: 두 사람의 의견을 반영하여 결정을 내릴 때, 한 사람의 의견을 더 높게 평가하면 그 사람의 의견이 최종 결정에 더 큰 영향을 미칩니다. Weighted Sum도 이와 유사하게 중요도가 높은 정보에 더 큰 비중을 둡니다.

다양한 텐서의 조합 확장
두 개의 텐서뿐 아니라 세 개, 네 개의 텐서로도 확장 가능합니다. 세 개의 웨이트가 비슷할 경우 결과는 중간 정보를 반영합니다.

예시: 세 사람의 의견을 모두 동일하게 존중할 때, 각 의견이 33%씩 반영되어 종합적인 결정을 내리게 됩니다. 텐서의 조합도 이와 유사합니다.

### 내적의 기본 성질
내적의 정의와 정보의 유사성
내적은 두 벡터의 같은 차원 요소를 곱한 후 더하는 연산으로, 두 정보 간의 유사도를 계산하는 데 사용됩니다. 관련된 정보끼리 내적하면 값이 크고, 연관이 적은 정보는 값이 작아집니다. 이는 어텐션 메커니즘의 근간이 됩니다.

예시: 만약 'apple'과 'fruit'라는 단어의 유사성을 계산하면 큰 값을 얻겠지만, 'apple'과 'car'는 내적 값이 작을 것입니다. 어텐션 메커니즘에서는 이 유사도를 바탕으로 정보의 가중치를 결정합니다.




### 웨이티드 섬의 의미와 익스포넨셜 활용
웨이티드 섬은 정보를 섞는 과정입니다. 내적 결과가 음수일 수 있으므로, 익스포넨셜(exponential)을 사용하여 양수로 변환합니다. 전체 합이 1이 되도록 각 웨이트를 조정하며, 이를 통해 쿼리와 키의 유사성을 비율로 나타내어 정보를 조합합니다.

예시: 여러 친구에게 여행 장소 추천을 받은 후, 각 추천 장소의 빈도를 비율로 변환하여 가장 많이 추천된 장소를 선택하는 과정과 유사합니다.


### 어텐션의 핵심

어텐션의 핵심은 중요한 정보에 더 큰 가중치를 부여하여 필요한 정보를 효과적으로 추출하는 것입니다. 구체적으로는 **쿼리(Query)**가 어떤 정보를 찾고자 할 때, **키(Key)**와의 유사도를 계산하여 가장 관련성 높은 정보만 강조하는 방식입니다. 이를 통해 정보 간의 관계를 파악하고, 쿼리에 맞는 정보를 가중합(Weighted Sum) 형태로 조합하여 결과를 생성합니다.

이 메커니즘은 특히 긴 문장에서 특정 단어가 문맥상 어떤 의미를 가지고 있는지 파악하거나, 중요한 정보를 우선시할 때 유용합니다. 예를 들어, 번역 모델에서는 현재 단어가 이전 문장과 어떻게 연결되는지 파악하고, 중요한 단어에 더 큰 비중을 주어 번역의 일관성을 높입니다.

요약:

- 유사도 계산: 쿼리와 키의 내적을 통해 유사도를 측정.
- 가중치 부여: 유사도가 높은 정보에 더 큰 가중치.
- 정보 조합: 가중합을 통해 최종적으로 중요한 정보만을 추출.

이를 통해 어텐션은 필요한 정보를 효율적으로 선택하고, 쿼리에 맞는 최적의 조합을 만들어주는 역할을 합니다.





### 어텐션의 한계와 동음 이의어 문제
**동음 이의어 문제**
어텐션만으로는 단어의 중의성을 해결하기 어렵습니다. 예를 들어 "인도"는 문맥에 따라 다른 의미로 해석될 수 있습니다. 이는 어텐션 메커니즘의 한계 중 하나로, 추후 포지셔널 인코딩과 같은 방법이 필요합니다.

예시: "인도를 여행하다"와 "책을 인도하다"에서 어텐션은 각각의 문맥을 인식하기 어려워합니다.

### 포지셔널 인코딩의 필요성
포지셔널 인코딩의 역할과 순서 정보
포지셔널 인코딩은 문장의 각 단어에 위치 정보를 추가하여 어텐션 계산 시 순서를 반영합니다. 예를 들어, 동일한 단어라도 문장 내 위치에 따라 다르게 해석될 수 있도록 돕습니다.

예시: "나는 밥을 먹고 공부를 했다"와 "공부를 하고 밥을 먹었다"는 순서에 따라 의미가 다르므로, 포지셔널 인코딩이 필요합니다.

# 셀프 어텐션(Self-Attention) 자세히 이해하기

셀프 어텐션(Self-Attention)은 텍스트 처리와 같은 자연어 처리 작업에서 **문장 내 단어들이 서로 어떤 관계를 맺고 있는지 파악하는 방법**입니다. 이 메커니즘은 텍스트의 모든 단어가 다른 단어들과 관계를 맺도록 하여, 각 단어에 **문맥적 정보를 부여**합니다. 특히, 트랜스포머(Transformer) 모델에서 셀프 어텐션은 문장의 모든 단어가 서로 상호작용할 수 있도록 하여, 모델이 단어의 문맥을 더 잘 이해하도록 도와줍니다.

## 1. 셀프 어텐션의 작동 방식

셀프 어텐션은 다음의 과정으로 작동합니다:

1. **쿼리, 키, 밸류 생성**:
   각 단어 벡터는 **쿼리(Query)**, **키(Key)**, **밸류(Value)** 벡터로 변환됩니다. 이 벡터들은 행렬 곱을 통해 생성되며, 모델이 학습을 통해 각 단어의 특성에 맞는 가중치를 자동으로 조정합니다.

2. **유사도 계산**:
   각 쿼리 벡터는 모든 키 벡터와 내적(dot product)을 수행하여 유사도를 계산합니다. **내적 결과값이 클수록 해당 단어들이 더 유사하거나 중요하다고 판단**합니다. 예를 들어, "고양이는 귀엽다"라는 문장에서 "고양이"와 "귀엽다" 간의 유사도가 높다면, 이 두 단어가 의미적으로 관련이 있다고 볼 수 있습니다.

3. **가중치 정규화**:
   계산된 유사도 값을 **소프트맥스(Softmax) 함수**를 통해 정규화하여 확률처럼 만들고, 이 값을 **가중치로 사용**합니다. 이렇게 하면 모든 단어가 서로 관계를 맺지만, 중요한 단어가 더 큰 가중치를 부여받습니다.

4. **가중합 계산**:
   각 단어의 밸류 벡터에 가중치를 곱해 가중합을 계산합니다. 이 과정은 **모든 단어의 문맥적 정보가 반영된 최종 벡터를 생성**합니다. 예를 들어, "고양이는 귀엽다"에서 "귀엽다"의 정보는 "고양이" 벡터에 반영되어, "고양이"가 문맥상 귀엽다는 특징을 가지게 됩니다.

## 2. 셀프 어텐션의 예시

**문장 예시**: "고양이는 작은 동물이다"

- "고양이"라는 단어를 기준으로 보면, "작은"과 "동물"이라는 단어가 중요하다고 판단될 수 있습니다.
- 셀프 어텐션은 "고양이"와 "작은," "고양이"와 "동물" 간의 유사도를 계산하여, "고양이"가 "작다"는 특징과 "동물"이라는 범주 속성을 모두 포함하도록 가중합을 만듭니다.

이렇게 셀프 어텐션을 통해 단어들은 각자의 문맥에 맞는 정보를 가지게 되고, 문장 내에서 더 자연스럽게 연결됩니다.

## 3. 셀프 어텐션의 장점

- **멀리 떨어진 단어 간의 관계도 파악 가능**: 셀프 어텐션은 문장 내에서 멀리 떨어져 있는 단어들 간의 관계도 쉽게 파악할 수 있습니다. 예를 들어, 문장 초반과 후반에 있는 단어들이 서로 연관될 때 유용합니다.
- **병렬화가 가능**: 이전의 순차적인 RNN이나 LSTM과 달리, 셀프 어텐션은 병렬 처리가 가능하여 대규모 데이터 학습 속도를 크게 높일 수 있습니다.
- **문맥 이해력 향상**: 모든 단어가 문장 내 다른 단어들과 관계를 맺게 되므로, 특정 단어가 문장에서 어떻게 쓰였는지에 대한 문맥 정보를 더욱 잘 반영할 수 있습니다.

## 4. 셀프 어텐션이 중요한 이유

셀프 어텐션은 **Transformer 모델의 핵심**으로, 특히 **BERT**나 **GPT** 같은 모델에서 강력한 성능을 발휘합니다. 셀프 어텐션은 문장의 모든 단어 간의 관계를 계산하여 문맥을 이해하기 때문에, 번역, 요약, 질문 응답 등 다양한 자연어 처리 작업에서 뛰어난 성능을 발휘합니다. 

**예시**: "은행에서 돈을 인출했다"와 "강가에 있는 은행을 걷고 있다"에서 "은행"이 각각 다른 의미를 가지는 것을 셀프 어텐션이 문맥에 맞게 파악할 수 있습니다. 앞 문장에서는 "돈"과의 관계를 통해 금융 기관을 의미하고, 뒤 문장에서는 "강가"와의 관계를 통해 물가를 의미한다고 판단하는 것입니다.

## 핵심 요약

- **쿼리, 키, 밸류**를 통해 단어 간의 유사도 및 중요도 계산
- **가중치 부여**로 문맥에 맞는 정보 추출
- 문장 내 모든 단어 간의 **관계**를 파악하여 문맥을 더 잘 이해

셀프 어텐션은 이처럼 문장 내 단어들이 서로 연관되는 방식을 통해 자연어의 문맥을 이해하고, 이를 바탕으로 더 높은 정확도의 결과를 도출할 수 있습니다.


# 마스킹(Masking)과 트랜스포머(Transformer) 자세히 이해하기

## 마스킹(Masking)

마스킹은 **모델이 특정 정보에 접근하지 못하도록 차단**하는 기법으로, 트랜스포머와 같은 모델에서 중요한 역할을 합니다. 특히 **미래의 단어를 차단하여 정보의 일관성을 유지**하고, 모델이 예측하는 과정에서 미래 정보를 보지 않도록 막아줍니다.

### 1. 마스킹의 필요성

마스킹은 특히 언어 모델에서 **순차적 예측**을 필요로 할 때 유용합니다. 예를 들어, 번역 모델에서 다음 단어를 예측할 때는 **미래 단어를 참조하지 않고 현재까지의 단어만 사용**해야 합니다. 이렇게 하면 학습 과정에서 모델이 다음 단어를 예측할 때 단어 간 순서와 맥락을 더 잘 유지할 수 있습니다.

### 2. 마스킹의 종류

- **패딩 마스크(Padding Mask)**: 입력 데이터에 길이 차이가 있을 때 사용합니다. 모든 문장을 동일한 길이로 맞추기 위해 패딩(빈칸)을 추가하는데, 이 패딩은 모델이 학습에 불필요한 정보를 보지 않도록 마스크로 차단합니다.
- **룩 어헤드 마스크(Look-Ahead Mask)**: 순차적인 예측이 필요한 상황에서, **미래 단어를 차단**하여 모델이 다음 단어를 예측할 때 미래 정보를 참고하지 않도록 합니다.

### 3. 마스킹의 구현 방법

- 내적(dot product) 계산 후, 미래 단어의 위치를 음수 무한대 값으로 변경하여 모델이 해당 정보를 무시하게 합니다. 이렇게 하면 소프트맥스에서 해당 위치의 가중치가 0이 되면서 무시됩니다.

**예시**: "I am going"이라는 문장에서 "going" 다음에 나올 단어를 예측할 때, "to school" 같은 미래 단어는 마스킹으로 차단되어 현재 단어와 이전 단어만을 기반으로 예측이 이루어집니다.

---

## 트랜스포머(Transformer)

트랜스포머는 **자연어 처리(NLP)**에서 혁신적인 모델로, **어텐션 메커니즘**을 통해 데이터를 처리하고, 특히 번역, 요약, 텍스트 생성 등 다양한 작업에서 뛰어난 성능을 발휘합니다. RNN이나 LSTM과 달리 순차적인 구조가 아니기 때문에 **병렬 처리**가 가능하고, 더 빠른 학습 속도를 제공합니다.

### 1. 트랜스포머의 구조

트랜스포머는 크게 **인코더(Encoder)**와 **디코더(Decoder)**의 두 부분으로 구성됩니다. 각각 여러 층으로 쌓여 있으며, 각 층은 **멀티 헤드 어텐션(Multi-Head Attention)**과 **피드 포워드 신경망(Feed Forward Neural Network)**으로 구성됩니다.

- **인코더**: 입력 문장을 분석하고, 중요한 정보들을 추출하여 **내부 표현**을 생성합니다.
- **디코더**: 인코더에서 생성된 내부 표현을 바탕으로 **출력 문장**을 생성합니다. 디코더는 이전 예측된 단어를 바탕으로 다음 단어를 예측합니다.

### 2. 트랜스포머의 핵심 요소

1. **멀티 헤드 어텐션(Multi-Head Attention)**:
   - 여러 개의 **어텐션 헤드**를 사용하여 서로 다른 어텐션을 병렬로 수행합니다.
   - 각각의 어텐션 헤드는 서로 다른 부분에 집중하여 더 풍부한 정보를 추출합니다.

2. **포지셔널 인코딩(Positional Encoding)**:
   - 트랜스포머는 순서가 없는 구조이기 때문에, **포지셔널 인코딩**을 통해 단어의 위치 정보를 추가하여 순서를 반영합니다.
   - 사인 함수와 코사인 함수의 주기로 위치 정보를 부여합니다.

3. **피드 포워드 네트워크(Feed Forward Network)**:
   - 각 인코더와 디코더 층 내부에서 어텐션의 결과를 추가로 처리하는 **완전 연결 신경망**입니다.
   - 입력을 더 복잡하게 변형하여 모델이 더 높은 수준의 패턴을 학습할 수 있도록 돕습니다.

4. **잔차 연결(Residual Connection)**과 **레이어 정규화(Layer Normalization)**:
   - 각 층마다 잔차 연결을 통해 이전의 정보를 보존하고, 레이어 정규화를 통해 안정적인 학습을 돕습니다.

### 3. 트랜스포머의 번역 과정

트랜스포머는 인코더-디코더 구조를 통해 번역을 수행합니다.

1. **인코더 과정**:
   - 입력 문장의 모든 단어가 인코더로 들어가고, **셀프 어텐션**을 통해 단어 간의 관계를 파악한 후, **포지셔널 인코딩**을 추가하여 문맥을 반영합니다.
   - 이를 바탕으로 중요한 정보를 추출하고 내부 표현을 생성합니다.

2. **디코더 과정**:
   - 디코더는 인코더의 출력과 이전의 예측된 단어들을 참고하여, 다음에 올 단어를 순차적으로 예측합니다.
   - 이때, **룩 어헤드 마스킹**을 적용하여 미래의 단어를 참조하지 않도록 합니다.

**예시**: 영어 문장 "I am a student"를 한국어로 번역할 때, 인코더는 "I am a student"의 모든 단어를 분석하고, 디코더는 이를 바탕으로 "나는 학생입니다"를 순차적으로 생성합니다.

---

### 트랜스포머의 장점과 한계

- **장점**:
  - **병렬 처리**가 가능하여 학습 속도가 빠릅니다.
  - **장기 의존성 문제 해결**: 어텐션 메커니즘을 통해 문장 내 멀리 떨어진 단어 간의 관계도 쉽게 파악합니다.
  - **문맥 이해력 향상**: 셀프 어텐션을 통해 문장 내 모든 단어들이 서로 영향을 미쳐 더 깊은 문맥을 이해합니다.

- **한계**:
  - **계산 비용이 높음**: 모든 단어 쌍에 대해 어텐션을 계산해야 하므로, 매우 긴 문장에서는 계산 비용이 큽니다.
  - **동음이의어 문제**: 문맥에 따라 의미가 달라질 수 있는 단어의 경우 어텐션만으로는 한계가 있을 수 있습니다.

---

### 핵심 요약

- **마스킹**: 미래의 단어 정보를 차단하여 예측의 일관성 유지.
- **트랜스포머 구조**: 인코더와 디코더로 구성된 어텐션 기반 모델.
- **멀티 헤드 어텐션과 포지셔널 인코딩**: 다양한 문맥 정보와 위치 정보를 반영.
- **장점**: 빠른 학습 속도, 병렬 처리 가능, 문맥 이해력 우수.

트랜스포머는 이러한 구조적 특성과 어텐션 메커니즘을 통해 자연어 처리에서 뛰어난 성능을 발휘하고, 번역, 텍스트 생성, 요약 등 다양한 작업에서 폭넓게 사용됩니다.

***

어텐션과 트랜스포머를 공부해봤다. 
학습 구조를 이해하는데 도움이 많이 되었고 아직 갈길이 멀다고 생각된다 ㅎ