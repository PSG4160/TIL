{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f881cb2-3f20-4b69-a92c-9db10b96ad0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "117134lines [00:01, 81749.62lines/s] \n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_19300\\555835201.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  reviews = pad_sequence([torch.tensor(r, dtype=torch.long) for r in reviews], batch_first=True)  # 정수형 텐서로 변환\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Batch 10/1465, Loss: 1.4572\n",
      "Epoch 1/10, Batch 20/1465, Loss: 1.4592\n",
      "Epoch 1/10, Batch 30/1465, Loss: 1.4692\n",
      "Epoch 1/10, Batch 40/1465, Loss: 1.4214\n",
      "Epoch 1/10, Batch 50/1465, Loss: 1.4436\n",
      "Epoch 1/10, Batch 60/1465, Loss: 1.5373\n",
      "Epoch 1/10, Batch 70/1465, Loss: 1.4278\n",
      "Epoch 1/10, Batch 80/1465, Loss: 1.3642\n",
      "Epoch 1/10, Batch 90/1465, Loss: 1.4601\n",
      "Epoch 1/10, Batch 100/1465, Loss: 1.4164\n",
      "Epoch 1/10, Batch 110/1465, Loss: 1.3366\n",
      "Epoch 1/10, Batch 120/1465, Loss: 1.2683\n",
      "Epoch 1/10, Batch 130/1465, Loss: 1.4337\n",
      "Epoch 1/10, Batch 140/1465, Loss: 1.5160\n",
      "Epoch 1/10, Batch 150/1465, Loss: 1.3988\n",
      "Epoch 1/10, Batch 160/1465, Loss: 1.4385\n",
      "Epoch 1/10, Batch 170/1465, Loss: 1.5602\n",
      "Epoch 1/10, Batch 180/1465, Loss: 1.4744\n",
      "Epoch 1/10, Batch 190/1465, Loss: 1.4234\n",
      "Epoch 1/10, Batch 200/1465, Loss: 1.4611\n",
      "Epoch 1/10, Batch 210/1465, Loss: 1.3939\n",
      "Epoch 1/10, Batch 220/1465, Loss: 1.4393\n",
      "Epoch 1/10, Batch 230/1465, Loss: 1.4920\n",
      "Epoch 1/10, Batch 240/1465, Loss: 1.4392\n",
      "Epoch 1/10, Batch 250/1465, Loss: 1.4173\n",
      "Epoch 1/10, Batch 260/1465, Loss: 1.3917\n",
      "Epoch 1/10, Batch 270/1465, Loss: 1.5098\n",
      "Epoch 1/10, Batch 280/1465, Loss: 1.4242\n",
      "Epoch 1/10, Batch 290/1465, Loss: 1.4069\n",
      "Epoch 1/10, Batch 300/1465, Loss: 1.5118\n",
      "Epoch 1/10, Batch 310/1465, Loss: 1.4212\n",
      "Epoch 1/10, Batch 320/1465, Loss: 1.4244\n",
      "Epoch 1/10, Batch 330/1465, Loss: 1.4758\n",
      "Epoch 1/10, Batch 340/1465, Loss: 1.4688\n",
      "Epoch 1/10, Batch 350/1465, Loss: 1.5065\n",
      "Epoch 1/10, Batch 360/1465, Loss: 1.4764\n",
      "Epoch 1/10, Batch 370/1465, Loss: 1.3661\n",
      "Epoch 1/10, Batch 380/1465, Loss: 1.4123\n",
      "Epoch 1/10, Batch 390/1465, Loss: 1.4727\n",
      "Epoch 1/10, Batch 400/1465, Loss: 1.5179\n",
      "Epoch 1/10, Batch 410/1465, Loss: 1.5018\n",
      "Epoch 1/10, Batch 420/1465, Loss: 1.4157\n",
      "Epoch 1/10, Batch 430/1465, Loss: 1.4353\n",
      "Epoch 1/10, Batch 440/1465, Loss: 1.5452\n",
      "Epoch 1/10, Batch 450/1465, Loss: 1.4636\n",
      "Epoch 1/10, Batch 460/1465, Loss: 1.2898\n",
      "Epoch 1/10, Batch 470/1465, Loss: 1.4253\n",
      "Epoch 1/10, Batch 480/1465, Loss: 1.5087\n",
      "Epoch 1/10, Batch 490/1465, Loss: 1.4404\n",
      "Epoch 1/10, Batch 500/1465, Loss: 1.4020\n",
      "Epoch 1/10, Batch 510/1465, Loss: 1.5592\n",
      "Epoch 1/10, Batch 520/1465, Loss: 1.4619\n",
      "Epoch 1/10, Batch 530/1465, Loss: 1.4924\n",
      "Epoch 1/10, Batch 540/1465, Loss: 1.4730\n",
      "Epoch 1/10, Batch 550/1465, Loss: 1.4161\n",
      "Epoch 1/10, Batch 560/1465, Loss: 1.4959\n",
      "Epoch 1/10, Batch 570/1465, Loss: 1.4392\n",
      "Epoch 1/10, Batch 580/1465, Loss: 1.4814\n",
      "Epoch 1/10, Batch 590/1465, Loss: 1.4292\n",
      "Epoch 1/10, Batch 600/1465, Loss: 1.4611\n",
      "Epoch 1/10, Batch 610/1465, Loss: 1.4003\n",
      "Epoch 1/10, Batch 620/1465, Loss: 1.4210\n",
      "Epoch 1/10, Batch 630/1465, Loss: 1.5225\n",
      "Epoch 1/10, Batch 640/1465, Loss: 1.4988\n",
      "Epoch 1/10, Batch 650/1465, Loss: 1.4673\n",
      "Epoch 1/10, Batch 660/1465, Loss: 1.3816\n",
      "Epoch 1/10, Batch 670/1465, Loss: 1.5217\n",
      "Epoch 1/10, Batch 680/1465, Loss: 1.2642\n",
      "Epoch 1/10, Batch 690/1465, Loss: 1.6517\n",
      "Epoch 1/10, Batch 700/1465, Loss: 1.3249\n",
      "Epoch 1/10, Batch 710/1465, Loss: 1.3725\n",
      "Epoch 1/10, Batch 720/1465, Loss: 1.3367\n",
      "Epoch 1/10, Batch 730/1465, Loss: 1.1192\n",
      "Epoch 1/10, Batch 740/1465, Loss: 1.0887\n",
      "Epoch 1/10, Batch 750/1465, Loss: 1.2283\n",
      "Epoch 1/10, Batch 760/1465, Loss: 1.1582\n",
      "Epoch 1/10, Batch 770/1465, Loss: 1.4255\n",
      "Epoch 1/10, Batch 780/1465, Loss: 1.4967\n",
      "Epoch 1/10, Batch 790/1465, Loss: 1.0907\n",
      "Epoch 1/10, Batch 800/1465, Loss: 1.1738\n",
      "Epoch 1/10, Batch 810/1465, Loss: 1.2154\n",
      "Epoch 1/10, Batch 820/1465, Loss: 1.2885\n",
      "Epoch 1/10, Batch 830/1465, Loss: 1.0957\n",
      "Epoch 1/10, Batch 840/1465, Loss: 1.0994\n",
      "Epoch 1/10, Batch 850/1465, Loss: 1.2164\n",
      "Epoch 1/10, Batch 860/1465, Loss: 1.2019\n",
      "Epoch 1/10, Batch 870/1465, Loss: 1.2041\n",
      "Epoch 1/10, Batch 880/1465, Loss: 1.2484\n",
      "Epoch 1/10, Batch 890/1465, Loss: 1.2750\n",
      "Epoch 1/10, Batch 900/1465, Loss: 1.2436\n",
      "Epoch 1/10, Batch 910/1465, Loss: 1.0319\n",
      "Epoch 1/10, Batch 920/1465, Loss: 1.3275\n",
      "Epoch 1/10, Batch 930/1465, Loss: 1.2578\n",
      "Epoch 1/10, Batch 940/1465, Loss: 1.1480\n",
      "Epoch 1/10, Batch 950/1465, Loss: 1.0490\n",
      "Epoch 1/10, Batch 960/1465, Loss: 1.1517\n",
      "Epoch 1/10, Batch 970/1465, Loss: 1.1135\n",
      "Epoch 1/10, Batch 980/1465, Loss: 0.9989\n",
      "Epoch 1/10, Batch 990/1465, Loss: 1.2657\n",
      "Epoch 1/10, Batch 1000/1465, Loss: 1.2096\n",
      "Epoch 1/10, Batch 1010/1465, Loss: 1.2808\n",
      "Epoch 1/10, Batch 1020/1465, Loss: 1.3459\n",
      "Epoch 1/10, Batch 1030/1465, Loss: 1.3125\n",
      "Epoch 1/10, Batch 1040/1465, Loss: 1.1095\n",
      "Epoch 1/10, Batch 1050/1465, Loss: 1.0433\n",
      "Epoch 1/10, Batch 1060/1465, Loss: 1.1732\n",
      "Epoch 1/10, Batch 1070/1465, Loss: 1.0759\n",
      "Epoch 1/10, Batch 1080/1465, Loss: 1.0286\n",
      "Epoch 1/10, Batch 1090/1465, Loss: 0.9964\n",
      "Epoch 1/10, Batch 1100/1465, Loss: 0.9269\n",
      "Epoch 1/10, Batch 1110/1465, Loss: 1.1066\n",
      "Epoch 1/10, Batch 1120/1465, Loss: 1.2727\n",
      "Epoch 1/10, Batch 1130/1465, Loss: 1.2745\n",
      "Epoch 1/10, Batch 1140/1465, Loss: 1.0954\n",
      "Epoch 1/10, Batch 1150/1465, Loss: 0.7235\n",
      "Epoch 1/10, Batch 1160/1465, Loss: 1.1932\n",
      "Epoch 1/10, Batch 1170/1465, Loss: 1.0436\n",
      "Epoch 1/10, Batch 1180/1465, Loss: 1.1226\n",
      "Epoch 1/10, Batch 1190/1465, Loss: 1.1185\n",
      "Epoch 1/10, Batch 1200/1465, Loss: 1.2191\n",
      "Epoch 1/10, Batch 1210/1465, Loss: 1.3092\n",
      "Epoch 1/10, Batch 1220/1465, Loss: 1.1150\n",
      "Epoch 1/10, Batch 1230/1465, Loss: 0.9939\n",
      "Epoch 1/10, Batch 1240/1465, Loss: 1.1584\n",
      "Epoch 1/10, Batch 1250/1465, Loss: 1.1408\n",
      "Epoch 1/10, Batch 1260/1465, Loss: 1.2218\n",
      "Epoch 1/10, Batch 1270/1465, Loss: 1.1336\n",
      "Epoch 1/10, Batch 1280/1465, Loss: 0.9633\n",
      "Epoch 1/10, Batch 1290/1465, Loss: 1.3166\n",
      "Epoch 1/10, Batch 1300/1465, Loss: 0.9877\n",
      "Epoch 1/10, Batch 1310/1465, Loss: 1.0421\n",
      "Epoch 1/10, Batch 1320/1465, Loss: 0.9223\n",
      "Epoch 1/10, Batch 1330/1465, Loss: 1.0705\n",
      "Epoch 1/10, Batch 1340/1465, Loss: 1.0470\n",
      "Epoch 1/10, Batch 1350/1465, Loss: 1.0597\n",
      "Epoch 1/10, Batch 1360/1465, Loss: 1.1301\n",
      "Epoch 1/10, Batch 1370/1465, Loss: 1.1351\n",
      "Epoch 1/10, Batch 1380/1465, Loss: 1.1684\n",
      "Epoch 1/10, Batch 1390/1465, Loss: 1.1265\n",
      "Epoch 1/10, Batch 1400/1465, Loss: 1.0045\n",
      "Epoch 1/10, Batch 1410/1465, Loss: 0.9188\n",
      "Epoch 1/10, Batch 1420/1465, Loss: 0.9349\n",
      "Epoch 1/10, Batch 1430/1465, Loss: 1.3378\n",
      "Epoch 1/10, Batch 1440/1465, Loss: 1.1543\n",
      "Epoch 1/10, Batch 1450/1465, Loss: 0.9817\n",
      "Epoch 1/10, Batch 1460/1465, Loss: 0.9471\n",
      "Epoch [1/10], Average Loss: 1.2862\n",
      "Epoch 2/10, Batch 10/1465, Loss: 0.9242\n",
      "Epoch 2/10, Batch 20/1465, Loss: 1.0746\n",
      "Epoch 2/10, Batch 30/1465, Loss: 0.9651\n",
      "Epoch 2/10, Batch 40/1465, Loss: 0.9352\n",
      "Epoch 2/10, Batch 50/1465, Loss: 0.8870\n",
      "Epoch 2/10, Batch 60/1465, Loss: 1.0466\n",
      "Epoch 2/10, Batch 70/1465, Loss: 1.0468\n",
      "Epoch 2/10, Batch 80/1465, Loss: 1.2201\n",
      "Epoch 2/10, Batch 90/1465, Loss: 1.0660\n",
      "Epoch 2/10, Batch 100/1465, Loss: 1.0836\n",
      "Epoch 2/10, Batch 110/1465, Loss: 1.1901\n",
      "Epoch 2/10, Batch 120/1465, Loss: 1.1904\n",
      "Epoch 2/10, Batch 130/1465, Loss: 0.8063\n",
      "Epoch 2/10, Batch 140/1465, Loss: 0.8996\n",
      "Epoch 2/10, Batch 150/1465, Loss: 1.2978\n",
      "Epoch 2/10, Batch 160/1465, Loss: 1.1833\n",
      "Epoch 2/10, Batch 170/1465, Loss: 0.9212\n",
      "Epoch 2/10, Batch 180/1465, Loss: 0.9677\n",
      "Epoch 2/10, Batch 190/1465, Loss: 0.9644\n",
      "Epoch 2/10, Batch 200/1465, Loss: 1.2813\n",
      "Epoch 2/10, Batch 210/1465, Loss: 1.0093\n",
      "Epoch 2/10, Batch 220/1465, Loss: 0.9960\n",
      "Epoch 2/10, Batch 230/1465, Loss: 1.1080\n",
      "Epoch 2/10, Batch 240/1465, Loss: 1.0212\n",
      "Epoch 2/10, Batch 250/1465, Loss: 1.0027\n",
      "Epoch 2/10, Batch 260/1465, Loss: 1.0014\n",
      "Epoch 2/10, Batch 270/1465, Loss: 0.8449\n",
      "Epoch 2/10, Batch 280/1465, Loss: 1.1532\n",
      "Epoch 2/10, Batch 290/1465, Loss: 1.0895\n",
      "Epoch 2/10, Batch 300/1465, Loss: 1.0187\n",
      "Epoch 2/10, Batch 310/1465, Loss: 0.9512\n",
      "Epoch 2/10, Batch 320/1465, Loss: 1.0567\n",
      "Epoch 2/10, Batch 330/1465, Loss: 0.9500\n",
      "Epoch 2/10, Batch 340/1465, Loss: 0.9379\n",
      "Epoch 2/10, Batch 350/1465, Loss: 1.0532\n",
      "Epoch 2/10, Batch 360/1465, Loss: 1.0049\n",
      "Epoch 2/10, Batch 370/1465, Loss: 0.9877\n",
      "Epoch 2/10, Batch 380/1465, Loss: 1.0664\n",
      "Epoch 2/10, Batch 390/1465, Loss: 1.0281\n",
      "Epoch 2/10, Batch 400/1465, Loss: 1.0467\n",
      "Epoch 2/10, Batch 410/1465, Loss: 1.0685\n",
      "Epoch 2/10, Batch 420/1465, Loss: 1.2419\n",
      "Epoch 2/10, Batch 430/1465, Loss: 1.0298\n",
      "Epoch 2/10, Batch 440/1465, Loss: 0.9904\n",
      "Epoch 2/10, Batch 450/1465, Loss: 0.8779\n",
      "Epoch 2/10, Batch 460/1465, Loss: 1.1542\n",
      "Epoch 2/10, Batch 470/1465, Loss: 0.9729\n",
      "Epoch 2/10, Batch 480/1465, Loss: 0.8086\n",
      "Epoch 2/10, Batch 490/1465, Loss: 0.9411\n",
      "Epoch 2/10, Batch 500/1465, Loss: 1.0907\n",
      "Epoch 2/10, Batch 510/1465, Loss: 0.9463\n",
      "Epoch 2/10, Batch 520/1465, Loss: 0.9738\n",
      "Epoch 2/10, Batch 530/1465, Loss: 1.0260\n",
      "Epoch 2/10, Batch 540/1465, Loss: 1.1748\n",
      "Epoch 2/10, Batch 550/1465, Loss: 1.1050\n",
      "Epoch 2/10, Batch 560/1465, Loss: 1.0201\n",
      "Epoch 2/10, Batch 570/1465, Loss: 1.0709\n",
      "Epoch 2/10, Batch 580/1465, Loss: 1.0648\n",
      "Epoch 2/10, Batch 590/1465, Loss: 1.0875\n",
      "Epoch 2/10, Batch 600/1465, Loss: 1.0298\n",
      "Epoch 2/10, Batch 610/1465, Loss: 1.1001\n",
      "Epoch 2/10, Batch 620/1465, Loss: 1.1162\n",
      "Epoch 2/10, Batch 630/1465, Loss: 1.0625\n",
      "Epoch 2/10, Batch 640/1465, Loss: 1.0736\n",
      "Epoch 2/10, Batch 650/1465, Loss: 1.0673\n",
      "Epoch 2/10, Batch 660/1465, Loss: 1.0139\n",
      "Epoch 2/10, Batch 670/1465, Loss: 1.1971\n",
      "Epoch 2/10, Batch 680/1465, Loss: 0.9144\n",
      "Epoch 2/10, Batch 690/1465, Loss: 0.9768\n",
      "Epoch 2/10, Batch 700/1465, Loss: 1.1521\n",
      "Epoch 2/10, Batch 710/1465, Loss: 1.0175\n",
      "Epoch 2/10, Batch 720/1465, Loss: 0.9471\n",
      "Epoch 2/10, Batch 730/1465, Loss: 1.1280\n",
      "Epoch 2/10, Batch 740/1465, Loss: 1.2741\n",
      "Epoch 2/10, Batch 750/1465, Loss: 1.2231\n",
      "Epoch 2/10, Batch 760/1465, Loss: 1.1919\n",
      "Epoch 2/10, Batch 770/1465, Loss: 1.0138\n",
      "Epoch 2/10, Batch 780/1465, Loss: 1.0647\n",
      "Epoch 2/10, Batch 790/1465, Loss: 0.7959\n",
      "Epoch 2/10, Batch 800/1465, Loss: 1.2252\n",
      "Epoch 2/10, Batch 810/1465, Loss: 0.9315\n",
      "Epoch 2/10, Batch 820/1465, Loss: 1.1842\n",
      "Epoch 2/10, Batch 830/1465, Loss: 1.0257\n",
      "Epoch 2/10, Batch 840/1465, Loss: 1.0581\n",
      "Epoch 2/10, Batch 850/1465, Loss: 1.1488\n",
      "Epoch 2/10, Batch 860/1465, Loss: 0.9633\n",
      "Epoch 2/10, Batch 870/1465, Loss: 1.3017\n",
      "Epoch 2/10, Batch 880/1465, Loss: 0.9507\n",
      "Epoch 2/10, Batch 890/1465, Loss: 1.2085\n",
      "Epoch 2/10, Batch 900/1465, Loss: 0.9797\n",
      "Epoch 2/10, Batch 910/1465, Loss: 1.0447\n",
      "Epoch 2/10, Batch 920/1465, Loss: 1.0907\n",
      "Epoch 2/10, Batch 930/1465, Loss: 1.2167\n",
      "Epoch 2/10, Batch 940/1465, Loss: 1.0806\n",
      "Epoch 2/10, Batch 950/1465, Loss: 0.9291\n",
      "Epoch 2/10, Batch 960/1465, Loss: 1.1124\n",
      "Epoch 2/10, Batch 970/1465, Loss: 1.1581\n",
      "Epoch 2/10, Batch 980/1465, Loss: 0.9764\n",
      "Epoch 2/10, Batch 990/1465, Loss: 1.0969\n",
      "Epoch 2/10, Batch 1000/1465, Loss: 0.9528\n",
      "Epoch 2/10, Batch 1010/1465, Loss: 1.1462\n",
      "Epoch 2/10, Batch 1020/1465, Loss: 1.1751\n",
      "Epoch 2/10, Batch 1030/1465, Loss: 0.9155\n",
      "Epoch 2/10, Batch 1040/1465, Loss: 0.8456\n",
      "Epoch 2/10, Batch 1050/1465, Loss: 1.1702\n",
      "Epoch 2/10, Batch 1060/1465, Loss: 0.9682\n",
      "Epoch 2/10, Batch 1070/1465, Loss: 1.2544\n",
      "Epoch 2/10, Batch 1080/1465, Loss: 0.9322\n",
      "Epoch 2/10, Batch 1090/1465, Loss: 1.1325\n",
      "Epoch 2/10, Batch 1100/1465, Loss: 1.0011\n",
      "Epoch 2/10, Batch 1110/1465, Loss: 0.9459\n",
      "Epoch 2/10, Batch 1120/1465, Loss: 0.9437\n",
      "Epoch 2/10, Batch 1130/1465, Loss: 1.0263\n",
      "Epoch 2/10, Batch 1140/1465, Loss: 0.9035\n",
      "Epoch 2/10, Batch 1150/1465, Loss: 1.0987\n",
      "Epoch 2/10, Batch 1160/1465, Loss: 1.0467\n",
      "Epoch 2/10, Batch 1170/1465, Loss: 1.1503\n",
      "Epoch 2/10, Batch 1180/1465, Loss: 1.0102\n",
      "Epoch 2/10, Batch 1190/1465, Loss: 1.1248\n",
      "Epoch 2/10, Batch 1200/1465, Loss: 1.0877\n",
      "Epoch 2/10, Batch 1210/1465, Loss: 1.0563\n",
      "Epoch 2/10, Batch 1220/1465, Loss: 1.0495\n",
      "Epoch 2/10, Batch 1230/1465, Loss: 0.9459\n",
      "Epoch 2/10, Batch 1240/1465, Loss: 1.3495\n",
      "Epoch 2/10, Batch 1250/1465, Loss: 1.1414\n",
      "Epoch 2/10, Batch 1260/1465, Loss: 0.8935\n",
      "Epoch 2/10, Batch 1270/1465, Loss: 1.1713\n",
      "Epoch 2/10, Batch 1280/1465, Loss: 1.0902\n",
      "Epoch 2/10, Batch 1290/1465, Loss: 1.1077\n",
      "Epoch 2/10, Batch 1300/1465, Loss: 0.9641\n",
      "Epoch 2/10, Batch 1310/1465, Loss: 1.0586\n",
      "Epoch 2/10, Batch 1320/1465, Loss: 1.1452\n",
      "Epoch 2/10, Batch 1330/1465, Loss: 1.2011\n",
      "Epoch 2/10, Batch 1340/1465, Loss: 0.9003\n",
      "Epoch 2/10, Batch 1350/1465, Loss: 1.1901\n",
      "Epoch 2/10, Batch 1360/1465, Loss: 1.1517\n",
      "Epoch 2/10, Batch 1370/1465, Loss: 0.9291\n",
      "Epoch 2/10, Batch 1380/1465, Loss: 1.1970\n",
      "Epoch 2/10, Batch 1390/1465, Loss: 0.9911\n",
      "Epoch 2/10, Batch 1400/1465, Loss: 1.1421\n",
      "Epoch 2/10, Batch 1410/1465, Loss: 0.9311\n",
      "Epoch 2/10, Batch 1420/1465, Loss: 1.0284\n",
      "Epoch 2/10, Batch 1430/1465, Loss: 1.1202\n",
      "Epoch 2/10, Batch 1440/1465, Loss: 1.0190\n",
      "Epoch 2/10, Batch 1450/1465, Loss: 1.0809\n",
      "Epoch 2/10, Batch 1460/1465, Loss: 1.0871\n",
      "Epoch [2/10], Average Loss: 1.0352\n",
      "Epoch 3/10, Batch 10/1465, Loss: 0.9409\n",
      "Epoch 3/10, Batch 20/1465, Loss: 1.0491\n",
      "Epoch 3/10, Batch 30/1465, Loss: 1.1888\n",
      "Epoch 3/10, Batch 40/1465, Loss: 1.0685\n",
      "Epoch 3/10, Batch 50/1465, Loss: 0.9420\n",
      "Epoch 3/10, Batch 60/1465, Loss: 1.1209\n",
      "Epoch 3/10, Batch 70/1465, Loss: 1.1728\n",
      "Epoch 3/10, Batch 80/1465, Loss: 0.8804\n",
      "Epoch 3/10, Batch 90/1465, Loss: 1.1719\n",
      "Epoch 3/10, Batch 100/1465, Loss: 0.8390\n",
      "Epoch 3/10, Batch 110/1465, Loss: 1.0305\n",
      "Epoch 3/10, Batch 120/1465, Loss: 1.0016\n",
      "Epoch 3/10, Batch 130/1465, Loss: 0.9272\n",
      "Epoch 3/10, Batch 140/1465, Loss: 0.9925\n",
      "Epoch 3/10, Batch 150/1465, Loss: 0.8298\n",
      "Epoch 3/10, Batch 160/1465, Loss: 1.1262\n",
      "Epoch 3/10, Batch 170/1465, Loss: 1.0838\n",
      "Epoch 3/10, Batch 180/1465, Loss: 0.8687\n",
      "Epoch 3/10, Batch 190/1465, Loss: 1.0260\n",
      "Epoch 3/10, Batch 200/1465, Loss: 1.0492\n",
      "Epoch 3/10, Batch 210/1465, Loss: 0.9796\n",
      "Epoch 3/10, Batch 220/1465, Loss: 0.9192\n",
      "Epoch 3/10, Batch 230/1465, Loss: 0.9667\n",
      "Epoch 3/10, Batch 240/1465, Loss: 0.7975\n",
      "Epoch 3/10, Batch 250/1465, Loss: 1.0704\n",
      "Epoch 3/10, Batch 260/1465, Loss: 0.6986\n",
      "Epoch 3/10, Batch 270/1465, Loss: 1.0823\n",
      "Epoch 3/10, Batch 280/1465, Loss: 0.8688\n",
      "Epoch 3/10, Batch 290/1465, Loss: 0.8901\n",
      "Epoch 3/10, Batch 300/1465, Loss: 0.8502\n",
      "Epoch 3/10, Batch 310/1465, Loss: 1.2255\n",
      "Epoch 3/10, Batch 320/1465, Loss: 0.8756\n",
      "Epoch 3/10, Batch 330/1465, Loss: 1.0083\n",
      "Epoch 3/10, Batch 340/1465, Loss: 1.0930\n",
      "Epoch 3/10, Batch 350/1465, Loss: 1.0851\n",
      "Epoch 3/10, Batch 360/1465, Loss: 1.0461\n",
      "Epoch 3/10, Batch 370/1465, Loss: 0.9795\n",
      "Epoch 3/10, Batch 380/1465, Loss: 0.8547\n",
      "Epoch 3/10, Batch 390/1465, Loss: 0.8597\n",
      "Epoch 3/10, Batch 400/1465, Loss: 1.1117\n",
      "Epoch 3/10, Batch 410/1465, Loss: 0.7715\n",
      "Epoch 3/10, Batch 420/1465, Loss: 1.0377\n",
      "Epoch 3/10, Batch 430/1465, Loss: 0.9118\n",
      "Epoch 3/10, Batch 440/1465, Loss: 1.0564\n",
      "Epoch 3/10, Batch 450/1465, Loss: 1.0283\n",
      "Epoch 3/10, Batch 460/1465, Loss: 0.9719\n",
      "Epoch 3/10, Batch 470/1465, Loss: 1.0221\n",
      "Epoch 3/10, Batch 480/1465, Loss: 0.9164\n",
      "Epoch 3/10, Batch 490/1465, Loss: 0.9895\n",
      "Epoch 3/10, Batch 500/1465, Loss: 1.1776\n",
      "Epoch 3/10, Batch 510/1465, Loss: 1.0289\n",
      "Epoch 3/10, Batch 520/1465, Loss: 0.9844\n",
      "Epoch 3/10, Batch 530/1465, Loss: 1.1327\n",
      "Epoch 3/10, Batch 540/1465, Loss: 1.1387\n",
      "Epoch 3/10, Batch 550/1465, Loss: 0.8935\n",
      "Epoch 3/10, Batch 560/1465, Loss: 0.9248\n",
      "Epoch 3/10, Batch 570/1465, Loss: 1.0188\n",
      "Epoch 3/10, Batch 580/1465, Loss: 0.8528\n",
      "Epoch 3/10, Batch 590/1465, Loss: 1.0743\n",
      "Epoch 3/10, Batch 600/1465, Loss: 0.9891\n",
      "Epoch 3/10, Batch 610/1465, Loss: 0.8358\n",
      "Epoch 3/10, Batch 620/1465, Loss: 1.0034\n",
      "Epoch 3/10, Batch 630/1465, Loss: 0.8562\n",
      "Epoch 3/10, Batch 640/1465, Loss: 0.8882\n",
      "Epoch 3/10, Batch 650/1465, Loss: 1.0728\n",
      "Epoch 3/10, Batch 660/1465, Loss: 1.0288\n",
      "Epoch 3/10, Batch 670/1465, Loss: 1.1401\n",
      "Epoch 3/10, Batch 680/1465, Loss: 1.4036\n",
      "Epoch 3/10, Batch 690/1465, Loss: 1.1345\n",
      "Epoch 3/10, Batch 700/1465, Loss: 0.9243\n",
      "Epoch 3/10, Batch 710/1465, Loss: 1.0374\n",
      "Epoch 3/10, Batch 720/1465, Loss: 0.8700\n",
      "Epoch 3/10, Batch 730/1465, Loss: 1.0407\n",
      "Epoch 3/10, Batch 740/1465, Loss: 1.2850\n",
      "Epoch 3/10, Batch 750/1465, Loss: 1.0020\n",
      "Epoch 3/10, Batch 760/1465, Loss: 0.9321\n",
      "Epoch 3/10, Batch 770/1465, Loss: 1.1058\n",
      "Epoch 3/10, Batch 780/1465, Loss: 1.0724\n",
      "Epoch 3/10, Batch 790/1465, Loss: 0.9430\n",
      "Epoch 3/10, Batch 800/1465, Loss: 1.0377\n",
      "Epoch 3/10, Batch 810/1465, Loss: 0.9805\n",
      "Epoch 3/10, Batch 820/1465, Loss: 1.0425\n",
      "Epoch 3/10, Batch 830/1465, Loss: 0.9467\n",
      "Epoch 3/10, Batch 840/1465, Loss: 1.0140\n",
      "Epoch 3/10, Batch 850/1465, Loss: 1.2273\n",
      "Epoch 3/10, Batch 860/1465, Loss: 0.9694\n",
      "Epoch 3/10, Batch 870/1465, Loss: 1.0329\n",
      "Epoch 3/10, Batch 880/1465, Loss: 1.1120\n",
      "Epoch 3/10, Batch 890/1465, Loss: 0.9011\n",
      "Epoch 3/10, Batch 900/1465, Loss: 0.8075\n",
      "Epoch 3/10, Batch 910/1465, Loss: 0.9286\n",
      "Epoch 3/10, Batch 920/1465, Loss: 1.0390\n",
      "Epoch 3/10, Batch 930/1465, Loss: 1.0910\n",
      "Epoch 3/10, Batch 940/1465, Loss: 0.9040\n",
      "Epoch 3/10, Batch 950/1465, Loss: 1.0460\n",
      "Epoch 3/10, Batch 960/1465, Loss: 0.8051\n",
      "Epoch 3/10, Batch 970/1465, Loss: 1.1047\n",
      "Epoch 3/10, Batch 980/1465, Loss: 1.1506\n",
      "Epoch 3/10, Batch 990/1465, Loss: 0.7081\n",
      "Epoch 3/10, Batch 1000/1465, Loss: 0.9051\n",
      "Epoch 3/10, Batch 1010/1465, Loss: 0.8305\n",
      "Epoch 3/10, Batch 1020/1465, Loss: 1.1160\n",
      "Epoch 3/10, Batch 1030/1465, Loss: 1.1053\n",
      "Epoch 3/10, Batch 1040/1465, Loss: 1.0114\n",
      "Epoch 3/10, Batch 1050/1465, Loss: 1.0534\n",
      "Epoch 3/10, Batch 1060/1465, Loss: 0.7534\n",
      "Epoch 3/10, Batch 1070/1465, Loss: 1.1537\n",
      "Epoch 3/10, Batch 1080/1465, Loss: 1.0973\n",
      "Epoch 3/10, Batch 1090/1465, Loss: 0.7806\n",
      "Epoch 3/10, Batch 1100/1465, Loss: 0.8580\n",
      "Epoch 3/10, Batch 1110/1465, Loss: 0.8962\n",
      "Epoch 3/10, Batch 1120/1465, Loss: 0.9307\n",
      "Epoch 3/10, Batch 1130/1465, Loss: 1.0322\n",
      "Epoch 3/10, Batch 1140/1465, Loss: 1.0473\n",
      "Epoch 3/10, Batch 1150/1465, Loss: 0.9752\n",
      "Epoch 3/10, Batch 1160/1465, Loss: 0.8226\n",
      "Epoch 3/10, Batch 1170/1465, Loss: 0.8824\n",
      "Epoch 3/10, Batch 1180/1465, Loss: 1.0572\n",
      "Epoch 3/10, Batch 1190/1465, Loss: 0.8667\n",
      "Epoch 3/10, Batch 1200/1465, Loss: 1.0304\n",
      "Epoch 3/10, Batch 1210/1465, Loss: 0.9940\n",
      "Epoch 3/10, Batch 1220/1465, Loss: 0.8550\n",
      "Epoch 3/10, Batch 1230/1465, Loss: 1.2002\n",
      "Epoch 3/10, Batch 1240/1465, Loss: 0.9388\n",
      "Epoch 3/10, Batch 1250/1465, Loss: 0.9658\n",
      "Epoch 3/10, Batch 1260/1465, Loss: 1.2149\n",
      "Epoch 3/10, Batch 1270/1465, Loss: 1.0167\n",
      "Epoch 3/10, Batch 1280/1465, Loss: 0.9997\n",
      "Epoch 3/10, Batch 1290/1465, Loss: 0.9470\n",
      "Epoch 3/10, Batch 1300/1465, Loss: 0.9762\n",
      "Epoch 3/10, Batch 1310/1465, Loss: 1.1176\n",
      "Epoch 3/10, Batch 1320/1465, Loss: 1.0167\n",
      "Epoch 3/10, Batch 1330/1465, Loss: 1.0903\n",
      "Epoch 3/10, Batch 1340/1465, Loss: 0.9907\n",
      "Epoch 3/10, Batch 1350/1465, Loss: 1.1586\n",
      "Epoch 3/10, Batch 1360/1465, Loss: 1.0700\n",
      "Epoch 3/10, Batch 1370/1465, Loss: 0.9946\n",
      "Epoch 3/10, Batch 1380/1465, Loss: 0.9146\n",
      "Epoch 3/10, Batch 1390/1465, Loss: 0.9340\n",
      "Epoch 3/10, Batch 1400/1465, Loss: 1.0013\n",
      "Epoch 3/10, Batch 1410/1465, Loss: 0.9437\n",
      "Epoch 3/10, Batch 1420/1465, Loss: 1.1733\n",
      "Epoch 3/10, Batch 1430/1465, Loss: 1.1877\n",
      "Epoch 3/10, Batch 1440/1465, Loss: 0.8664\n",
      "Epoch 3/10, Batch 1450/1465, Loss: 0.8219\n",
      "Epoch 3/10, Batch 1460/1465, Loss: 1.2052\n",
      "Epoch [3/10], Average Loss: 0.9881\n",
      "Epoch 4/10, Batch 10/1465, Loss: 1.2075\n",
      "Epoch 4/10, Batch 20/1465, Loss: 0.9859\n",
      "Epoch 4/10, Batch 30/1465, Loss: 0.9225\n",
      "Epoch 4/10, Batch 40/1465, Loss: 1.0530\n",
      "Epoch 4/10, Batch 50/1465, Loss: 1.0519\n",
      "Epoch 4/10, Batch 60/1465, Loss: 1.1535\n",
      "Epoch 4/10, Batch 70/1465, Loss: 0.9598\n",
      "Epoch 4/10, Batch 80/1465, Loss: 0.9424\n",
      "Epoch 4/10, Batch 90/1465, Loss: 1.0365\n",
      "Epoch 4/10, Batch 100/1465, Loss: 0.8996\n",
      "Epoch 4/10, Batch 110/1465, Loss: 1.0475\n",
      "Epoch 4/10, Batch 120/1465, Loss: 0.9857\n",
      "Epoch 4/10, Batch 130/1465, Loss: 0.8364\n",
      "Epoch 4/10, Batch 140/1465, Loss: 1.1850\n",
      "Epoch 4/10, Batch 150/1465, Loss: 1.1842\n",
      "Epoch 4/10, Batch 160/1465, Loss: 0.9063\n",
      "Epoch 4/10, Batch 170/1465, Loss: 0.9575\n",
      "Epoch 4/10, Batch 180/1465, Loss: 1.0572\n",
      "Epoch 4/10, Batch 190/1465, Loss: 0.7918\n",
      "Epoch 4/10, Batch 200/1465, Loss: 0.8654\n",
      "Epoch 4/10, Batch 210/1465, Loss: 0.9664\n",
      "Epoch 4/10, Batch 220/1465, Loss: 0.9518\n",
      "Epoch 4/10, Batch 230/1465, Loss: 0.9799\n",
      "Epoch 4/10, Batch 240/1465, Loss: 1.1167\n",
      "Epoch 4/10, Batch 250/1465, Loss: 1.1403\n",
      "Epoch 4/10, Batch 260/1465, Loss: 1.0132\n",
      "Epoch 4/10, Batch 270/1465, Loss: 0.7729\n",
      "Epoch 4/10, Batch 280/1465, Loss: 0.8139\n",
      "Epoch 4/10, Batch 290/1465, Loss: 0.8396\n",
      "Epoch 4/10, Batch 300/1465, Loss: 0.8539\n",
      "Epoch 4/10, Batch 310/1465, Loss: 0.8874\n",
      "Epoch 4/10, Batch 320/1465, Loss: 1.1011\n",
      "Epoch 4/10, Batch 330/1465, Loss: 1.0453\n",
      "Epoch 4/10, Batch 340/1465, Loss: 0.7908\n",
      "Epoch 4/10, Batch 350/1465, Loss: 0.9770\n",
      "Epoch 4/10, Batch 360/1465, Loss: 0.9907\n",
      "Epoch 4/10, Batch 370/1465, Loss: 1.0811\n",
      "Epoch 4/10, Batch 380/1465, Loss: 0.7227\n",
      "Epoch 4/10, Batch 390/1465, Loss: 0.8797\n",
      "Epoch 4/10, Batch 400/1465, Loss: 0.7513\n",
      "Epoch 4/10, Batch 410/1465, Loss: 0.8645\n",
      "Epoch 4/10, Batch 420/1465, Loss: 1.0610\n",
      "Epoch 4/10, Batch 430/1465, Loss: 1.0965\n",
      "Epoch 4/10, Batch 440/1465, Loss: 1.0755\n",
      "Epoch 4/10, Batch 450/1465, Loss: 0.9601\n",
      "Epoch 4/10, Batch 460/1465, Loss: 1.0886\n",
      "Epoch 4/10, Batch 470/1465, Loss: 0.7139\n",
      "Epoch 4/10, Batch 480/1465, Loss: 0.8856\n",
      "Epoch 4/10, Batch 490/1465, Loss: 0.9422\n",
      "Epoch 4/10, Batch 500/1465, Loss: 0.9743\n",
      "Epoch 4/10, Batch 510/1465, Loss: 0.8394\n",
      "Epoch 4/10, Batch 520/1465, Loss: 1.0820\n",
      "Epoch 4/10, Batch 530/1465, Loss: 0.7110\n",
      "Epoch 4/10, Batch 540/1465, Loss: 1.0698\n",
      "Epoch 4/10, Batch 550/1465, Loss: 1.0247\n",
      "Epoch 4/10, Batch 560/1465, Loss: 0.9106\n",
      "Epoch 4/10, Batch 570/1465, Loss: 0.7659\n",
      "Epoch 4/10, Batch 580/1465, Loss: 1.1161\n",
      "Epoch 4/10, Batch 590/1465, Loss: 0.8776\n",
      "Epoch 4/10, Batch 600/1465, Loss: 1.0693\n",
      "Epoch 4/10, Batch 610/1465, Loss: 1.0265\n",
      "Epoch 4/10, Batch 620/1465, Loss: 1.0981\n",
      "Epoch 4/10, Batch 630/1465, Loss: 0.7424\n",
      "Epoch 4/10, Batch 640/1465, Loss: 1.0110\n",
      "Epoch 4/10, Batch 650/1465, Loss: 0.8639\n",
      "Epoch 4/10, Batch 660/1465, Loss: 0.9681\n",
      "Epoch 4/10, Batch 670/1465, Loss: 0.8577\n",
      "Epoch 4/10, Batch 680/1465, Loss: 0.9154\n",
      "Epoch 4/10, Batch 690/1465, Loss: 1.0980\n",
      "Epoch 4/10, Batch 700/1465, Loss: 0.9026\n",
      "Epoch 4/10, Batch 710/1465, Loss: 1.1591\n",
      "Epoch 4/10, Batch 720/1465, Loss: 0.9068\n",
      "Epoch 4/10, Batch 730/1465, Loss: 0.8394\n",
      "Epoch 4/10, Batch 740/1465, Loss: 0.9787\n",
      "Epoch 4/10, Batch 750/1465, Loss: 0.8988\n",
      "Epoch 4/10, Batch 760/1465, Loss: 1.1757\n",
      "Epoch 4/10, Batch 770/1465, Loss: 1.0687\n",
      "Epoch 4/10, Batch 780/1465, Loss: 0.7519\n",
      "Epoch 4/10, Batch 790/1465, Loss: 1.0337\n",
      "Epoch 4/10, Batch 800/1465, Loss: 0.9973\n",
      "Epoch 4/10, Batch 810/1465, Loss: 1.0178\n",
      "Epoch 4/10, Batch 820/1465, Loss: 0.9368\n",
      "Epoch 4/10, Batch 830/1465, Loss: 0.7479\n",
      "Epoch 4/10, Batch 840/1465, Loss: 1.1359\n",
      "Epoch 4/10, Batch 850/1465, Loss: 0.7769\n",
      "Epoch 4/10, Batch 860/1465, Loss: 1.0429\n",
      "Epoch 4/10, Batch 870/1465, Loss: 0.9899\n",
      "Epoch 4/10, Batch 880/1465, Loss: 0.8776\n",
      "Epoch 4/10, Batch 890/1465, Loss: 0.8724\n",
      "Epoch 4/10, Batch 900/1465, Loss: 0.8740\n",
      "Epoch 4/10, Batch 910/1465, Loss: 1.0043\n",
      "Epoch 4/10, Batch 920/1465, Loss: 0.9743\n",
      "Epoch 4/10, Batch 930/1465, Loss: 0.8655\n",
      "Epoch 4/10, Batch 940/1465, Loss: 0.8009\n",
      "Epoch 4/10, Batch 950/1465, Loss: 1.0059\n",
      "Epoch 4/10, Batch 960/1465, Loss: 0.9277\n",
      "Epoch 4/10, Batch 970/1465, Loss: 0.9679\n",
      "Epoch 4/10, Batch 980/1465, Loss: 1.1562\n",
      "Epoch 4/10, Batch 990/1465, Loss: 0.9189\n",
      "Epoch 4/10, Batch 1000/1465, Loss: 0.9349\n",
      "Epoch 4/10, Batch 1010/1465, Loss: 1.0194\n",
      "Epoch 4/10, Batch 1020/1465, Loss: 1.0751\n",
      "Epoch 4/10, Batch 1030/1465, Loss: 0.8827\n",
      "Epoch 4/10, Batch 1040/1465, Loss: 0.8191\n",
      "Epoch 4/10, Batch 1050/1465, Loss: 0.9219\n",
      "Epoch 4/10, Batch 1060/1465, Loss: 1.1568\n",
      "Epoch 4/10, Batch 1070/1465, Loss: 1.0447\n",
      "Epoch 4/10, Batch 1080/1465, Loss: 1.1120\n",
      "Epoch 4/10, Batch 1090/1465, Loss: 1.0190\n",
      "Epoch 4/10, Batch 1100/1465, Loss: 0.9472\n",
      "Epoch 4/10, Batch 1110/1465, Loss: 0.8193\n",
      "Epoch 4/10, Batch 1120/1465, Loss: 0.8694\n",
      "Epoch 4/10, Batch 1130/1465, Loss: 1.0305\n",
      "Epoch 4/10, Batch 1140/1465, Loss: 0.9757\n",
      "Epoch 4/10, Batch 1150/1465, Loss: 0.9777\n",
      "Epoch 4/10, Batch 1160/1465, Loss: 0.8015\n",
      "Epoch 4/10, Batch 1170/1465, Loss: 0.9578\n",
      "Epoch 4/10, Batch 1180/1465, Loss: 0.8808\n",
      "Epoch 4/10, Batch 1190/1465, Loss: 0.9201\n",
      "Epoch 4/10, Batch 1200/1465, Loss: 0.8224\n",
      "Epoch 4/10, Batch 1210/1465, Loss: 0.8511\n",
      "Epoch 4/10, Batch 1220/1465, Loss: 0.9687\n",
      "Epoch 4/10, Batch 1230/1465, Loss: 0.8354\n",
      "Epoch 4/10, Batch 1240/1465, Loss: 0.7436\n",
      "Epoch 4/10, Batch 1250/1465, Loss: 0.9402\n",
      "Epoch 4/10, Batch 1260/1465, Loss: 1.1847\n",
      "Epoch 4/10, Batch 1270/1465, Loss: 1.1118\n",
      "Epoch 4/10, Batch 1280/1465, Loss: 1.0486\n",
      "Epoch 4/10, Batch 1290/1465, Loss: 0.8775\n",
      "Epoch 4/10, Batch 1300/1465, Loss: 1.2114\n",
      "Epoch 4/10, Batch 1310/1465, Loss: 0.9282\n",
      "Epoch 4/10, Batch 1320/1465, Loss: 0.9215\n",
      "Epoch 4/10, Batch 1330/1465, Loss: 0.9834\n",
      "Epoch 4/10, Batch 1340/1465, Loss: 1.0714\n",
      "Epoch 4/10, Batch 1350/1465, Loss: 0.8388\n",
      "Epoch 4/10, Batch 1360/1465, Loss: 1.1257\n",
      "Epoch 4/10, Batch 1370/1465, Loss: 1.0444\n",
      "Epoch 4/10, Batch 1380/1465, Loss: 0.8844\n",
      "Epoch 4/10, Batch 1390/1465, Loss: 0.9044\n",
      "Epoch 4/10, Batch 1400/1465, Loss: 0.8765\n",
      "Epoch 4/10, Batch 1410/1465, Loss: 1.0720\n",
      "Epoch 4/10, Batch 1420/1465, Loss: 0.8970\n",
      "Epoch 4/10, Batch 1430/1465, Loss: 1.0450\n",
      "Epoch 4/10, Batch 1440/1465, Loss: 0.9666\n",
      "Epoch 4/10, Batch 1450/1465, Loss: 0.9804\n",
      "Epoch 4/10, Batch 1460/1465, Loss: 0.9570\n",
      "Epoch [4/10], Average Loss: 0.9620\n",
      "Epoch 5/10, Batch 10/1465, Loss: 0.8411\n",
      "Epoch 5/10, Batch 20/1465, Loss: 0.8430\n",
      "Epoch 5/10, Batch 30/1465, Loss: 0.8717\n",
      "Epoch 5/10, Batch 40/1465, Loss: 1.0970\n",
      "Epoch 5/10, Batch 50/1465, Loss: 0.8797\n",
      "Epoch 5/10, Batch 60/1465, Loss: 0.9549\n",
      "Epoch 5/10, Batch 70/1465, Loss: 0.7767\n",
      "Epoch 5/10, Batch 80/1465, Loss: 0.8514\n",
      "Epoch 5/10, Batch 90/1465, Loss: 0.9631\n",
      "Epoch 5/10, Batch 100/1465, Loss: 1.0404\n",
      "Epoch 5/10, Batch 110/1465, Loss: 0.8617\n",
      "Epoch 5/10, Batch 120/1465, Loss: 0.7242\n",
      "Epoch 5/10, Batch 130/1465, Loss: 1.0339\n",
      "Epoch 5/10, Batch 140/1465, Loss: 0.8885\n",
      "Epoch 5/10, Batch 150/1465, Loss: 0.9187\n",
      "Epoch 5/10, Batch 160/1465, Loss: 1.2325\n",
      "Epoch 5/10, Batch 170/1465, Loss: 1.0510\n",
      "Epoch 5/10, Batch 180/1465, Loss: 1.1919\n",
      "Epoch 5/10, Batch 190/1465, Loss: 0.9174\n",
      "Epoch 5/10, Batch 200/1465, Loss: 1.0802\n",
      "Epoch 5/10, Batch 210/1465, Loss: 0.8705\n",
      "Epoch 5/10, Batch 220/1465, Loss: 1.0722\n",
      "Epoch 5/10, Batch 230/1465, Loss: 0.9966\n",
      "Epoch 5/10, Batch 240/1465, Loss: 0.8207\n",
      "Epoch 5/10, Batch 250/1465, Loss: 0.9860\n",
      "Epoch 5/10, Batch 260/1465, Loss: 0.9128\n",
      "Epoch 5/10, Batch 270/1465, Loss: 0.8577\n",
      "Epoch 5/10, Batch 280/1465, Loss: 0.7775\n",
      "Epoch 5/10, Batch 290/1465, Loss: 0.8295\n",
      "Epoch 5/10, Batch 300/1465, Loss: 1.0988\n",
      "Epoch 5/10, Batch 310/1465, Loss: 1.1083\n",
      "Epoch 5/10, Batch 320/1465, Loss: 0.7491\n",
      "Epoch 5/10, Batch 330/1465, Loss: 0.9103\n",
      "Epoch 5/10, Batch 340/1465, Loss: 0.9664\n",
      "Epoch 5/10, Batch 350/1465, Loss: 0.7537\n",
      "Epoch 5/10, Batch 360/1465, Loss: 1.0866\n",
      "Epoch 5/10, Batch 370/1465, Loss: 1.0649\n",
      "Epoch 5/10, Batch 380/1465, Loss: 0.7501\n",
      "Epoch 5/10, Batch 390/1465, Loss: 0.8387\n",
      "Epoch 5/10, Batch 400/1465, Loss: 1.1335\n",
      "Epoch 5/10, Batch 410/1465, Loss: 0.9916\n",
      "Epoch 5/10, Batch 420/1465, Loss: 0.9317\n",
      "Epoch 5/10, Batch 430/1465, Loss: 0.8202\n",
      "Epoch 5/10, Batch 440/1465, Loss: 0.8320\n",
      "Epoch 5/10, Batch 450/1465, Loss: 1.0258\n",
      "Epoch 5/10, Batch 460/1465, Loss: 1.0824\n",
      "Epoch 5/10, Batch 470/1465, Loss: 0.7079\n",
      "Epoch 5/10, Batch 480/1465, Loss: 1.0053\n",
      "Epoch 5/10, Batch 490/1465, Loss: 1.1187\n",
      "Epoch 5/10, Batch 500/1465, Loss: 0.9620\n",
      "Epoch 5/10, Batch 510/1465, Loss: 0.9516\n",
      "Epoch 5/10, Batch 520/1465, Loss: 0.7824\n",
      "Epoch 5/10, Batch 530/1465, Loss: 0.9525\n",
      "Epoch 5/10, Batch 540/1465, Loss: 0.9215\n",
      "Epoch 5/10, Batch 550/1465, Loss: 1.0035\n",
      "Epoch 5/10, Batch 560/1465, Loss: 1.0309\n",
      "Epoch 5/10, Batch 570/1465, Loss: 0.8954\n",
      "Epoch 5/10, Batch 580/1465, Loss: 1.1101\n",
      "Epoch 5/10, Batch 590/1465, Loss: 0.8915\n",
      "Epoch 5/10, Batch 600/1465, Loss: 0.8668\n",
      "Epoch 5/10, Batch 610/1465, Loss: 0.8023\n",
      "Epoch 5/10, Batch 620/1465, Loss: 0.9936\n",
      "Epoch 5/10, Batch 630/1465, Loss: 1.0077\n",
      "Epoch 5/10, Batch 640/1465, Loss: 0.8410\n",
      "Epoch 5/10, Batch 650/1465, Loss: 0.9252\n",
      "Epoch 5/10, Batch 660/1465, Loss: 0.9841\n",
      "Epoch 5/10, Batch 670/1465, Loss: 0.7551\n",
      "Epoch 5/10, Batch 680/1465, Loss: 1.1305\n",
      "Epoch 5/10, Batch 690/1465, Loss: 0.7294\n",
      "Epoch 5/10, Batch 700/1465, Loss: 1.1765\n",
      "Epoch 5/10, Batch 710/1465, Loss: 0.8014\n",
      "Epoch 5/10, Batch 720/1465, Loss: 1.0481\n",
      "Epoch 5/10, Batch 730/1465, Loss: 0.8807\n",
      "Epoch 5/10, Batch 740/1465, Loss: 1.1033\n",
      "Epoch 5/10, Batch 750/1465, Loss: 1.0441\n",
      "Epoch 5/10, Batch 760/1465, Loss: 1.1509\n",
      "Epoch 5/10, Batch 770/1465, Loss: 1.0680\n",
      "Epoch 5/10, Batch 780/1465, Loss: 0.8712\n",
      "Epoch 5/10, Batch 790/1465, Loss: 0.7538\n",
      "Epoch 5/10, Batch 800/1465, Loss: 1.0481\n",
      "Epoch 5/10, Batch 810/1465, Loss: 1.0498\n",
      "Epoch 5/10, Batch 820/1465, Loss: 0.8490\n",
      "Epoch 5/10, Batch 830/1465, Loss: 0.9791\n",
      "Epoch 5/10, Batch 840/1465, Loss: 1.0630\n",
      "Epoch 5/10, Batch 850/1465, Loss: 1.0871\n",
      "Epoch 5/10, Batch 860/1465, Loss: 0.9724\n",
      "Epoch 5/10, Batch 870/1465, Loss: 1.0163\n",
      "Epoch 5/10, Batch 880/1465, Loss: 0.8609\n",
      "Epoch 5/10, Batch 890/1465, Loss: 1.3370\n",
      "Epoch 5/10, Batch 900/1465, Loss: 1.0106\n",
      "Epoch 5/10, Batch 910/1465, Loss: 0.8321\n",
      "Epoch 5/10, Batch 920/1465, Loss: 0.9545\n",
      "Epoch 5/10, Batch 930/1465, Loss: 1.0290\n",
      "Epoch 5/10, Batch 940/1465, Loss: 0.9919\n",
      "Epoch 5/10, Batch 950/1465, Loss: 0.9876\n",
      "Epoch 5/10, Batch 960/1465, Loss: 0.9857\n",
      "Epoch 5/10, Batch 970/1465, Loss: 0.8125\n",
      "Epoch 5/10, Batch 980/1465, Loss: 0.9463\n",
      "Epoch 5/10, Batch 990/1465, Loss: 1.0247\n",
      "Epoch 5/10, Batch 1000/1465, Loss: 0.9738\n",
      "Epoch 5/10, Batch 1010/1465, Loss: 0.9259\n",
      "Epoch 5/10, Batch 1020/1465, Loss: 0.8618\n",
      "Epoch 5/10, Batch 1030/1465, Loss: 0.7739\n",
      "Epoch 5/10, Batch 1040/1465, Loss: 0.9542\n",
      "Epoch 5/10, Batch 1050/1465, Loss: 0.8686\n",
      "Epoch 5/10, Batch 1060/1465, Loss: 0.8985\n",
      "Epoch 5/10, Batch 1070/1465, Loss: 0.9074\n",
      "Epoch 5/10, Batch 1080/1465, Loss: 0.7997\n",
      "Epoch 5/10, Batch 1090/1465, Loss: 1.0655\n",
      "Epoch 5/10, Batch 1100/1465, Loss: 0.9044\n",
      "Epoch 5/10, Batch 1110/1465, Loss: 0.8278\n",
      "Epoch 5/10, Batch 1120/1465, Loss: 0.9559\n",
      "Epoch 5/10, Batch 1130/1465, Loss: 0.9244\n",
      "Epoch 5/10, Batch 1140/1465, Loss: 1.0742\n",
      "Epoch 5/10, Batch 1150/1465, Loss: 0.8839\n",
      "Epoch 5/10, Batch 1160/1465, Loss: 1.0136\n",
      "Epoch 5/10, Batch 1170/1465, Loss: 0.9734\n",
      "Epoch 5/10, Batch 1180/1465, Loss: 0.8699\n",
      "Epoch 5/10, Batch 1190/1465, Loss: 1.0509\n",
      "Epoch 5/10, Batch 1200/1465, Loss: 0.9572\n",
      "Epoch 5/10, Batch 1210/1465, Loss: 0.9348\n",
      "Epoch 5/10, Batch 1220/1465, Loss: 0.8636\n",
      "Epoch 5/10, Batch 1230/1465, Loss: 0.8858\n",
      "Epoch 5/10, Batch 1240/1465, Loss: 0.8823\n",
      "Epoch 5/10, Batch 1250/1465, Loss: 1.0616\n",
      "Epoch 5/10, Batch 1260/1465, Loss: 1.2241\n",
      "Epoch 5/10, Batch 1270/1465, Loss: 0.9120\n",
      "Epoch 5/10, Batch 1280/1465, Loss: 0.8659\n",
      "Epoch 5/10, Batch 1290/1465, Loss: 1.1839\n",
      "Epoch 5/10, Batch 1300/1465, Loss: 1.0314\n",
      "Epoch 5/10, Batch 1310/1465, Loss: 0.9441\n",
      "Epoch 5/10, Batch 1320/1465, Loss: 0.9412\n",
      "Epoch 5/10, Batch 1330/1465, Loss: 0.8686\n",
      "Epoch 5/10, Batch 1340/1465, Loss: 1.0155\n",
      "Epoch 5/10, Batch 1350/1465, Loss: 0.8310\n",
      "Epoch 5/10, Batch 1360/1465, Loss: 0.9015\n",
      "Epoch 5/10, Batch 1370/1465, Loss: 1.0523\n",
      "Epoch 5/10, Batch 1380/1465, Loss: 0.9269\n",
      "Epoch 5/10, Batch 1390/1465, Loss: 0.9196\n",
      "Epoch 5/10, Batch 1400/1465, Loss: 1.2743\n",
      "Epoch 5/10, Batch 1410/1465, Loss: 0.7935\n",
      "Epoch 5/10, Batch 1420/1465, Loss: 0.7256\n",
      "Epoch 5/10, Batch 1430/1465, Loss: 0.8424\n",
      "Epoch 5/10, Batch 1440/1465, Loss: 0.9753\n",
      "Epoch 5/10, Batch 1450/1465, Loss: 1.1115\n",
      "Epoch 5/10, Batch 1460/1465, Loss: 1.0843\n",
      "Epoch [5/10], Average Loss: 0.9404\n",
      "Epoch 6/10, Batch 10/1465, Loss: 0.8751\n",
      "Epoch 6/10, Batch 20/1465, Loss: 0.8740\n",
      "Epoch 6/10, Batch 30/1465, Loss: 0.7649\n",
      "Epoch 6/10, Batch 40/1465, Loss: 0.8429\n",
      "Epoch 6/10, Batch 50/1465, Loss: 0.8962\n",
      "Epoch 6/10, Batch 60/1465, Loss: 0.9378\n",
      "Epoch 6/10, Batch 70/1465, Loss: 1.1233\n",
      "Epoch 6/10, Batch 80/1465, Loss: 1.0490\n",
      "Epoch 6/10, Batch 90/1465, Loss: 0.7800\n",
      "Epoch 6/10, Batch 100/1465, Loss: 1.0636\n",
      "Epoch 6/10, Batch 110/1465, Loss: 0.8288\n",
      "Epoch 6/10, Batch 120/1465, Loss: 0.7793\n",
      "Epoch 6/10, Batch 130/1465, Loss: 1.0038\n",
      "Epoch 6/10, Batch 140/1465, Loss: 0.9168\n",
      "Epoch 6/10, Batch 150/1465, Loss: 0.9460\n",
      "Epoch 6/10, Batch 160/1465, Loss: 0.8472\n",
      "Epoch 6/10, Batch 170/1465, Loss: 0.7918\n",
      "Epoch 6/10, Batch 180/1465, Loss: 1.1626\n",
      "Epoch 6/10, Batch 190/1465, Loss: 0.8523\n",
      "Epoch 6/10, Batch 200/1465, Loss: 1.2215\n",
      "Epoch 6/10, Batch 210/1465, Loss: 1.0211\n",
      "Epoch 6/10, Batch 220/1465, Loss: 0.8977\n",
      "Epoch 6/10, Batch 230/1465, Loss: 0.9329\n",
      "Epoch 6/10, Batch 240/1465, Loss: 0.8129\n",
      "Epoch 6/10, Batch 250/1465, Loss: 0.9064\n",
      "Epoch 6/10, Batch 260/1465, Loss: 0.8371\n",
      "Epoch 6/10, Batch 270/1465, Loss: 0.9159\n",
      "Epoch 6/10, Batch 280/1465, Loss: 0.8607\n",
      "Epoch 6/10, Batch 290/1465, Loss: 0.9925\n",
      "Epoch 6/10, Batch 300/1465, Loss: 1.0841\n",
      "Epoch 6/10, Batch 310/1465, Loss: 1.3070\n",
      "Epoch 6/10, Batch 320/1465, Loss: 0.9274\n",
      "Epoch 6/10, Batch 330/1465, Loss: 0.8683\n",
      "Epoch 6/10, Batch 340/1465, Loss: 0.8145\n",
      "Epoch 6/10, Batch 350/1465, Loss: 0.9755\n",
      "Epoch 6/10, Batch 360/1465, Loss: 0.8622\n",
      "Epoch 6/10, Batch 370/1465, Loss: 0.7741\n",
      "Epoch 6/10, Batch 380/1465, Loss: 1.1653\n",
      "Epoch 6/10, Batch 390/1465, Loss: 0.9499\n",
      "Epoch 6/10, Batch 400/1465, Loss: 1.0962\n",
      "Epoch 6/10, Batch 410/1465, Loss: 0.9991\n",
      "Epoch 6/10, Batch 420/1465, Loss: 0.9732\n",
      "Epoch 6/10, Batch 430/1465, Loss: 0.9713\n",
      "Epoch 6/10, Batch 440/1465, Loss: 0.7255\n",
      "Epoch 6/10, Batch 450/1465, Loss: 0.9709\n",
      "Epoch 6/10, Batch 460/1465, Loss: 0.7252\n",
      "Epoch 6/10, Batch 470/1465, Loss: 0.9161\n",
      "Epoch 6/10, Batch 480/1465, Loss: 1.0388\n",
      "Epoch 6/10, Batch 490/1465, Loss: 0.9994\n",
      "Epoch 6/10, Batch 500/1465, Loss: 1.1143\n",
      "Epoch 6/10, Batch 510/1465, Loss: 0.9428\n",
      "Epoch 6/10, Batch 520/1465, Loss: 0.9101\n",
      "Epoch 6/10, Batch 530/1465, Loss: 0.9236\n",
      "Epoch 6/10, Batch 540/1465, Loss: 0.8908\n",
      "Epoch 6/10, Batch 550/1465, Loss: 1.0481\n",
      "Epoch 6/10, Batch 560/1465, Loss: 1.0164\n",
      "Epoch 6/10, Batch 570/1465, Loss: 0.6982\n",
      "Epoch 6/10, Batch 580/1465, Loss: 0.9230\n",
      "Epoch 6/10, Batch 590/1465, Loss: 0.9880\n",
      "Epoch 6/10, Batch 600/1465, Loss: 1.0290\n",
      "Epoch 6/10, Batch 610/1465, Loss: 1.0033\n",
      "Epoch 6/10, Batch 620/1465, Loss: 0.7966\n",
      "Epoch 6/10, Batch 630/1465, Loss: 0.8633\n",
      "Epoch 6/10, Batch 640/1465, Loss: 0.6391\n",
      "Epoch 6/10, Batch 650/1465, Loss: 0.8521\n",
      "Epoch 6/10, Batch 660/1465, Loss: 0.9875\n",
      "Epoch 6/10, Batch 670/1465, Loss: 0.8572\n",
      "Epoch 6/10, Batch 680/1465, Loss: 0.8509\n",
      "Epoch 6/10, Batch 690/1465, Loss: 0.8390\n",
      "Epoch 6/10, Batch 700/1465, Loss: 0.7361\n",
      "Epoch 6/10, Batch 710/1465, Loss: 1.2506\n",
      "Epoch 6/10, Batch 720/1465, Loss: 0.9238\n",
      "Epoch 6/10, Batch 730/1465, Loss: 0.8257\n",
      "Epoch 6/10, Batch 740/1465, Loss: 0.9551\n",
      "Epoch 6/10, Batch 750/1465, Loss: 0.9091\n",
      "Epoch 6/10, Batch 760/1465, Loss: 0.9865\n",
      "Epoch 6/10, Batch 770/1465, Loss: 0.9890\n",
      "Epoch 6/10, Batch 780/1465, Loss: 1.0097\n",
      "Epoch 6/10, Batch 790/1465, Loss: 0.7599\n",
      "Epoch 6/10, Batch 800/1465, Loss: 1.0649\n",
      "Epoch 6/10, Batch 810/1465, Loss: 0.9019\n",
      "Epoch 6/10, Batch 820/1465, Loss: 0.8354\n",
      "Epoch 6/10, Batch 830/1465, Loss: 0.8085\n",
      "Epoch 6/10, Batch 840/1465, Loss: 0.9183\n",
      "Epoch 6/10, Batch 850/1465, Loss: 1.0742\n",
      "Epoch 6/10, Batch 860/1465, Loss: 0.7398\n",
      "Epoch 6/10, Batch 870/1465, Loss: 0.8009\n",
      "Epoch 6/10, Batch 880/1465, Loss: 1.0082\n",
      "Epoch 6/10, Batch 890/1465, Loss: 0.8804\n",
      "Epoch 6/10, Batch 900/1465, Loss: 1.0403\n",
      "Epoch 6/10, Batch 910/1465, Loss: 0.8760\n",
      "Epoch 6/10, Batch 920/1465, Loss: 0.6314\n",
      "Epoch 6/10, Batch 930/1465, Loss: 0.8532\n",
      "Epoch 6/10, Batch 940/1465, Loss: 1.0586\n",
      "Epoch 6/10, Batch 950/1465, Loss: 1.0776\n",
      "Epoch 6/10, Batch 960/1465, Loss: 0.9276\n",
      "Epoch 6/10, Batch 970/1465, Loss: 0.9419\n",
      "Epoch 6/10, Batch 980/1465, Loss: 1.1537\n",
      "Epoch 6/10, Batch 990/1465, Loss: 0.8121\n",
      "Epoch 6/10, Batch 1000/1465, Loss: 0.7844\n",
      "Epoch 6/10, Batch 1010/1465, Loss: 0.9993\n",
      "Epoch 6/10, Batch 1020/1465, Loss: 0.9985\n",
      "Epoch 6/10, Batch 1030/1465, Loss: 0.8611\n",
      "Epoch 6/10, Batch 1040/1465, Loss: 0.9558\n",
      "Epoch 6/10, Batch 1050/1465, Loss: 0.8565\n",
      "Epoch 6/10, Batch 1060/1465, Loss: 1.1043\n",
      "Epoch 6/10, Batch 1070/1465, Loss: 1.0315\n",
      "Epoch 6/10, Batch 1080/1465, Loss: 0.9370\n",
      "Epoch 6/10, Batch 1090/1465, Loss: 1.0095\n",
      "Epoch 6/10, Batch 1100/1465, Loss: 0.9126\n",
      "Epoch 6/10, Batch 1110/1465, Loss: 0.9065\n",
      "Epoch 6/10, Batch 1120/1465, Loss: 0.7658\n",
      "Epoch 6/10, Batch 1130/1465, Loss: 1.2125\n",
      "Epoch 6/10, Batch 1140/1465, Loss: 0.9781\n",
      "Epoch 6/10, Batch 1150/1465, Loss: 0.8828\n",
      "Epoch 6/10, Batch 1160/1465, Loss: 0.9308\n",
      "Epoch 6/10, Batch 1170/1465, Loss: 1.0081\n",
      "Epoch 6/10, Batch 1180/1465, Loss: 1.1380\n",
      "Epoch 6/10, Batch 1190/1465, Loss: 1.0335\n",
      "Epoch 6/10, Batch 1200/1465, Loss: 0.7773\n",
      "Epoch 6/10, Batch 1210/1465, Loss: 0.9752\n",
      "Epoch 6/10, Batch 1220/1465, Loss: 0.9071\n",
      "Epoch 6/10, Batch 1230/1465, Loss: 1.0802\n",
      "Epoch 6/10, Batch 1240/1465, Loss: 1.0061\n",
      "Epoch 6/10, Batch 1250/1465, Loss: 0.9335\n",
      "Epoch 6/10, Batch 1260/1465, Loss: 0.9863\n",
      "Epoch 6/10, Batch 1270/1465, Loss: 0.9618\n",
      "Epoch 6/10, Batch 1280/1465, Loss: 0.9879\n",
      "Epoch 6/10, Batch 1290/1465, Loss: 1.0212\n",
      "Epoch 6/10, Batch 1300/1465, Loss: 0.7624\n",
      "Epoch 6/10, Batch 1310/1465, Loss: 0.9722\n",
      "Epoch 6/10, Batch 1320/1465, Loss: 1.1689\n",
      "Epoch 6/10, Batch 1330/1465, Loss: 0.7215\n",
      "Epoch 6/10, Batch 1340/1465, Loss: 0.8579\n",
      "Epoch 6/10, Batch 1350/1465, Loss: 0.9124\n",
      "Epoch 6/10, Batch 1360/1465, Loss: 0.9801\n",
      "Epoch 6/10, Batch 1370/1465, Loss: 0.9862\n",
      "Epoch 6/10, Batch 1380/1465, Loss: 0.9228\n",
      "Epoch 6/10, Batch 1390/1465, Loss: 0.9328\n",
      "Epoch 6/10, Batch 1400/1465, Loss: 0.8631\n",
      "Epoch 6/10, Batch 1410/1465, Loss: 1.0378\n",
      "Epoch 6/10, Batch 1420/1465, Loss: 0.9099\n",
      "Epoch 6/10, Batch 1430/1465, Loss: 1.0217\n",
      "Epoch 6/10, Batch 1440/1465, Loss: 1.1540\n",
      "Epoch 6/10, Batch 1450/1465, Loss: 1.0989\n",
      "Epoch 6/10, Batch 1460/1465, Loss: 0.8918\n",
      "Epoch [6/10], Average Loss: 0.9284\n",
      "Epoch 7/10, Batch 10/1465, Loss: 1.0208\n",
      "Epoch 7/10, Batch 20/1465, Loss: 0.9819\n",
      "Epoch 7/10, Batch 30/1465, Loss: 0.8505\n",
      "Epoch 7/10, Batch 40/1465, Loss: 0.8830\n",
      "Epoch 7/10, Batch 50/1465, Loss: 0.9098\n",
      "Epoch 7/10, Batch 60/1465, Loss: 0.8118\n",
      "Epoch 7/10, Batch 70/1465, Loss: 0.9871\n",
      "Epoch 7/10, Batch 80/1465, Loss: 0.9459\n",
      "Epoch 7/10, Batch 90/1465, Loss: 0.8309\n",
      "Epoch 7/10, Batch 100/1465, Loss: 0.9155\n",
      "Epoch 7/10, Batch 110/1465, Loss: 0.6526\n",
      "Epoch 7/10, Batch 120/1465, Loss: 1.1038\n",
      "Epoch 7/10, Batch 130/1465, Loss: 0.7889\n",
      "Epoch 7/10, Batch 140/1465, Loss: 0.9254\n",
      "Epoch 7/10, Batch 150/1465, Loss: 0.9455\n",
      "Epoch 7/10, Batch 160/1465, Loss: 1.2348\n",
      "Epoch 7/10, Batch 170/1465, Loss: 0.9480\n",
      "Epoch 7/10, Batch 180/1465, Loss: 0.6821\n",
      "Epoch 7/10, Batch 190/1465, Loss: 1.0854\n",
      "Epoch 7/10, Batch 200/1465, Loss: 0.9241\n",
      "Epoch 7/10, Batch 210/1465, Loss: 1.0115\n",
      "Epoch 7/10, Batch 220/1465, Loss: 1.0678\n",
      "Epoch 7/10, Batch 230/1465, Loss: 0.8076\n",
      "Epoch 7/10, Batch 240/1465, Loss: 0.8960\n",
      "Epoch 7/10, Batch 250/1465, Loss: 0.9728\n",
      "Epoch 7/10, Batch 260/1465, Loss: 0.9733\n",
      "Epoch 7/10, Batch 270/1465, Loss: 0.9894\n",
      "Epoch 7/10, Batch 280/1465, Loss: 1.0644\n",
      "Epoch 7/10, Batch 290/1465, Loss: 0.7889\n",
      "Epoch 7/10, Batch 300/1465, Loss: 0.9748\n",
      "Epoch 7/10, Batch 310/1465, Loss: 0.8144\n",
      "Epoch 7/10, Batch 320/1465, Loss: 0.9873\n",
      "Epoch 7/10, Batch 330/1465, Loss: 0.7503\n",
      "Epoch 7/10, Batch 340/1465, Loss: 1.0944\n",
      "Epoch 7/10, Batch 350/1465, Loss: 0.8545\n",
      "Epoch 7/10, Batch 360/1465, Loss: 0.8060\n",
      "Epoch 7/10, Batch 370/1465, Loss: 1.0035\n",
      "Epoch 7/10, Batch 380/1465, Loss: 1.0028\n",
      "Epoch 7/10, Batch 390/1465, Loss: 1.0754\n",
      "Epoch 7/10, Batch 400/1465, Loss: 1.0054\n",
      "Epoch 7/10, Batch 410/1465, Loss: 1.1577\n",
      "Epoch 7/10, Batch 420/1465, Loss: 0.7390\n",
      "Epoch 7/10, Batch 430/1465, Loss: 0.8497\n",
      "Epoch 7/10, Batch 440/1465, Loss: 0.9440\n",
      "Epoch 7/10, Batch 450/1465, Loss: 0.9977\n",
      "Epoch 7/10, Batch 460/1465, Loss: 0.7249\n",
      "Epoch 7/10, Batch 470/1465, Loss: 0.8134\n",
      "Epoch 7/10, Batch 480/1465, Loss: 0.9009\n",
      "Epoch 7/10, Batch 490/1465, Loss: 0.9689\n",
      "Epoch 7/10, Batch 500/1465, Loss: 1.0753\n",
      "Epoch 7/10, Batch 510/1465, Loss: 1.2187\n",
      "Epoch 7/10, Batch 520/1465, Loss: 0.9593\n",
      "Epoch 7/10, Batch 530/1465, Loss: 1.0335\n",
      "Epoch 7/10, Batch 540/1465, Loss: 0.8364\n",
      "Epoch 7/10, Batch 550/1465, Loss: 1.1764\n",
      "Epoch 7/10, Batch 560/1465, Loss: 0.8231\n",
      "Epoch 7/10, Batch 570/1465, Loss: 0.7802\n",
      "Epoch 7/10, Batch 580/1465, Loss: 0.9448\n",
      "Epoch 7/10, Batch 590/1465, Loss: 1.0988\n",
      "Epoch 7/10, Batch 600/1465, Loss: 0.6341\n",
      "Epoch 7/10, Batch 610/1465, Loss: 0.9795\n",
      "Epoch 7/10, Batch 620/1465, Loss: 1.0647\n",
      "Epoch 7/10, Batch 630/1465, Loss: 0.9643\n",
      "Epoch 7/10, Batch 640/1465, Loss: 1.0343\n",
      "Epoch 7/10, Batch 650/1465, Loss: 0.8230\n",
      "Epoch 7/10, Batch 660/1465, Loss: 0.8770\n",
      "Epoch 7/10, Batch 670/1465, Loss: 0.9828\n",
      "Epoch 7/10, Batch 680/1465, Loss: 0.8344\n",
      "Epoch 7/10, Batch 690/1465, Loss: 1.0111\n",
      "Epoch 7/10, Batch 700/1465, Loss: 0.7497\n",
      "Epoch 7/10, Batch 710/1465, Loss: 0.9806\n",
      "Epoch 7/10, Batch 720/1465, Loss: 0.9441\n",
      "Epoch 7/10, Batch 730/1465, Loss: 1.0507\n",
      "Epoch 7/10, Batch 740/1465, Loss: 0.9306\n",
      "Epoch 7/10, Batch 750/1465, Loss: 0.7733\n",
      "Epoch 7/10, Batch 760/1465, Loss: 0.9099\n",
      "Epoch 7/10, Batch 770/1465, Loss: 1.0384\n",
      "Epoch 7/10, Batch 780/1465, Loss: 0.8581\n",
      "Epoch 7/10, Batch 790/1465, Loss: 0.9128\n",
      "Epoch 7/10, Batch 800/1465, Loss: 1.0095\n",
      "Epoch 7/10, Batch 810/1465, Loss: 0.9184\n",
      "Epoch 7/10, Batch 820/1465, Loss: 1.0792\n",
      "Epoch 7/10, Batch 830/1465, Loss: 0.8038\n",
      "Epoch 7/10, Batch 840/1465, Loss: 0.7875\n",
      "Epoch 7/10, Batch 850/1465, Loss: 0.9653\n",
      "Epoch 7/10, Batch 860/1465, Loss: 0.8746\n",
      "Epoch 7/10, Batch 870/1465, Loss: 0.9361\n",
      "Epoch 7/10, Batch 880/1465, Loss: 0.8682\n",
      "Epoch 7/10, Batch 890/1465, Loss: 1.0225\n",
      "Epoch 7/10, Batch 900/1465, Loss: 1.0057\n",
      "Epoch 7/10, Batch 910/1465, Loss: 0.9816\n",
      "Epoch 7/10, Batch 920/1465, Loss: 0.9451\n",
      "Epoch 7/10, Batch 930/1465, Loss: 1.1177\n",
      "Epoch 7/10, Batch 940/1465, Loss: 0.7049\n",
      "Epoch 7/10, Batch 950/1465, Loss: 1.0391\n",
      "Epoch 7/10, Batch 960/1465, Loss: 0.8827\n",
      "Epoch 7/10, Batch 970/1465, Loss: 0.8624\n",
      "Epoch 7/10, Batch 980/1465, Loss: 0.9268\n",
      "Epoch 7/10, Batch 990/1465, Loss: 1.0594\n",
      "Epoch 7/10, Batch 1000/1465, Loss: 1.3444\n",
      "Epoch 7/10, Batch 1010/1465, Loss: 1.0767\n",
      "Epoch 7/10, Batch 1020/1465, Loss: 0.9315\n",
      "Epoch 7/10, Batch 1030/1465, Loss: 0.9205\n",
      "Epoch 7/10, Batch 1040/1465, Loss: 0.8302\n",
      "Epoch 7/10, Batch 1050/1465, Loss: 0.8140\n",
      "Epoch 7/10, Batch 1060/1465, Loss: 1.0007\n",
      "Epoch 7/10, Batch 1070/1465, Loss: 0.7005\n",
      "Epoch 7/10, Batch 1080/1465, Loss: 0.8161\n",
      "Epoch 7/10, Batch 1090/1465, Loss: 0.9874\n",
      "Epoch 7/10, Batch 1100/1465, Loss: 0.6489\n",
      "Epoch 7/10, Batch 1110/1465, Loss: 1.0136\n",
      "Epoch 7/10, Batch 1120/1465, Loss: 0.8506\n",
      "Epoch 7/10, Batch 1130/1465, Loss: 0.7099\n",
      "Epoch 7/10, Batch 1140/1465, Loss: 0.9844\n",
      "Epoch 7/10, Batch 1150/1465, Loss: 0.8350\n",
      "Epoch 7/10, Batch 1160/1465, Loss: 0.8675\n",
      "Epoch 7/10, Batch 1170/1465, Loss: 0.9195\n",
      "Epoch 7/10, Batch 1180/1465, Loss: 0.8048\n",
      "Epoch 7/10, Batch 1190/1465, Loss: 0.8965\n",
      "Epoch 7/10, Batch 1200/1465, Loss: 1.0274\n",
      "Epoch 7/10, Batch 1210/1465, Loss: 0.8887\n",
      "Epoch 7/10, Batch 1220/1465, Loss: 0.8834\n",
      "Epoch 7/10, Batch 1230/1465, Loss: 0.8005\n",
      "Epoch 7/10, Batch 1240/1465, Loss: 0.9238\n",
      "Epoch 7/10, Batch 1250/1465, Loss: 1.0977\n",
      "Epoch 7/10, Batch 1260/1465, Loss: 0.8212\n",
      "Epoch 7/10, Batch 1270/1465, Loss: 0.8354\n",
      "Epoch 7/10, Batch 1280/1465, Loss: 0.8864\n",
      "Epoch 7/10, Batch 1290/1465, Loss: 0.8188\n",
      "Epoch 7/10, Batch 1300/1465, Loss: 1.0021\n",
      "Epoch 7/10, Batch 1310/1465, Loss: 0.9159\n",
      "Epoch 7/10, Batch 1320/1465, Loss: 0.8030\n",
      "Epoch 7/10, Batch 1330/1465, Loss: 0.9058\n",
      "Epoch 7/10, Batch 1340/1465, Loss: 0.8859\n",
      "Epoch 7/10, Batch 1350/1465, Loss: 1.1774\n",
      "Epoch 7/10, Batch 1360/1465, Loss: 0.8374\n",
      "Epoch 7/10, Batch 1370/1465, Loss: 1.0406\n",
      "Epoch 7/10, Batch 1380/1465, Loss: 0.9405\n",
      "Epoch 7/10, Batch 1390/1465, Loss: 0.7605\n",
      "Epoch 7/10, Batch 1400/1465, Loss: 0.9065\n",
      "Epoch 7/10, Batch 1410/1465, Loss: 0.7776\n",
      "Epoch 7/10, Batch 1420/1465, Loss: 0.7526\n",
      "Epoch 7/10, Batch 1430/1465, Loss: 1.1719\n",
      "Epoch 7/10, Batch 1440/1465, Loss: 1.1939\n",
      "Epoch 7/10, Batch 1450/1465, Loss: 0.9553\n",
      "Epoch 7/10, Batch 1460/1465, Loss: 0.8172\n",
      "Epoch [7/10], Average Loss: 0.9156\n",
      "Epoch 8/10, Batch 10/1465, Loss: 0.8316\n",
      "Epoch 8/10, Batch 20/1465, Loss: 0.6148\n",
      "Epoch 8/10, Batch 30/1465, Loss: 0.7661\n",
      "Epoch 8/10, Batch 40/1465, Loss: 1.0601\n",
      "Epoch 8/10, Batch 50/1465, Loss: 0.9192\n",
      "Epoch 8/10, Batch 60/1465, Loss: 0.8772\n",
      "Epoch 8/10, Batch 70/1465, Loss: 0.8471\n",
      "Epoch 8/10, Batch 80/1465, Loss: 1.2919\n",
      "Epoch 8/10, Batch 90/1465, Loss: 0.8933\n",
      "Epoch 8/10, Batch 100/1465, Loss: 0.8684\n",
      "Epoch 8/10, Batch 110/1465, Loss: 0.9208\n",
      "Epoch 8/10, Batch 120/1465, Loss: 0.8677\n",
      "Epoch 8/10, Batch 130/1465, Loss: 1.0152\n",
      "Epoch 8/10, Batch 140/1465, Loss: 1.0688\n",
      "Epoch 8/10, Batch 150/1465, Loss: 0.7939\n",
      "Epoch 8/10, Batch 160/1465, Loss: 0.7177\n",
      "Epoch 8/10, Batch 170/1465, Loss: 0.9642\n",
      "Epoch 8/10, Batch 180/1465, Loss: 0.8464\n",
      "Epoch 8/10, Batch 190/1465, Loss: 1.1973\n",
      "Epoch 8/10, Batch 200/1465, Loss: 0.7920\n",
      "Epoch 8/10, Batch 210/1465, Loss: 1.0771\n",
      "Epoch 8/10, Batch 220/1465, Loss: 0.9838\n",
      "Epoch 8/10, Batch 230/1465, Loss: 0.7835\n",
      "Epoch 8/10, Batch 240/1465, Loss: 0.8687\n",
      "Epoch 8/10, Batch 250/1465, Loss: 0.8348\n",
      "Epoch 8/10, Batch 260/1465, Loss: 1.1173\n",
      "Epoch 8/10, Batch 270/1465, Loss: 0.7767\n",
      "Epoch 8/10, Batch 280/1465, Loss: 0.8093\n",
      "Epoch 8/10, Batch 290/1465, Loss: 0.6788\n",
      "Epoch 8/10, Batch 300/1465, Loss: 0.7422\n",
      "Epoch 8/10, Batch 310/1465, Loss: 1.0227\n",
      "Epoch 8/10, Batch 320/1465, Loss: 0.8538\n",
      "Epoch 8/10, Batch 330/1465, Loss: 0.9554\n",
      "Epoch 8/10, Batch 340/1465, Loss: 0.8018\n",
      "Epoch 8/10, Batch 350/1465, Loss: 0.9535\n",
      "Epoch 8/10, Batch 360/1465, Loss: 0.9106\n",
      "Epoch 8/10, Batch 370/1465, Loss: 0.9667\n",
      "Epoch 8/10, Batch 380/1465, Loss: 0.9943\n",
      "Epoch 8/10, Batch 390/1465, Loss: 0.8092\n",
      "Epoch 8/10, Batch 400/1465, Loss: 0.8709\n",
      "Epoch 8/10, Batch 410/1465, Loss: 1.0561\n",
      "Epoch 8/10, Batch 420/1465, Loss: 0.9690\n",
      "Epoch 8/10, Batch 430/1465, Loss: 0.7893\n",
      "Epoch 8/10, Batch 440/1465, Loss: 0.9353\n",
      "Epoch 8/10, Batch 450/1465, Loss: 0.8683\n",
      "Epoch 8/10, Batch 460/1465, Loss: 0.7351\n",
      "Epoch 8/10, Batch 470/1465, Loss: 1.0265\n",
      "Epoch 8/10, Batch 480/1465, Loss: 1.0334\n",
      "Epoch 8/10, Batch 490/1465, Loss: 1.1889\n",
      "Epoch 8/10, Batch 500/1465, Loss: 0.7541\n",
      "Epoch 8/10, Batch 510/1465, Loss: 0.6599\n",
      "Epoch 8/10, Batch 520/1465, Loss: 0.9539\n",
      "Epoch 8/10, Batch 530/1465, Loss: 1.0791\n",
      "Epoch 8/10, Batch 540/1465, Loss: 0.9455\n",
      "Epoch 8/10, Batch 550/1465, Loss: 0.8804\n",
      "Epoch 8/10, Batch 560/1465, Loss: 0.9062\n",
      "Epoch 8/10, Batch 570/1465, Loss: 1.0383\n",
      "Epoch 8/10, Batch 580/1465, Loss: 1.0291\n",
      "Epoch 8/10, Batch 590/1465, Loss: 0.8470\n",
      "Epoch 8/10, Batch 600/1465, Loss: 1.0403\n",
      "Epoch 8/10, Batch 610/1465, Loss: 0.9878\n",
      "Epoch 8/10, Batch 620/1465, Loss: 0.9491\n",
      "Epoch 8/10, Batch 630/1465, Loss: 1.0217\n",
      "Epoch 8/10, Batch 640/1465, Loss: 0.8873\n",
      "Epoch 8/10, Batch 650/1465, Loss: 0.8397\n",
      "Epoch 8/10, Batch 660/1465, Loss: 0.7486\n",
      "Epoch 8/10, Batch 670/1465, Loss: 0.8495\n",
      "Epoch 8/10, Batch 680/1465, Loss: 1.1310\n",
      "Epoch 8/10, Batch 690/1465, Loss: 1.0152\n",
      "Epoch 8/10, Batch 700/1465, Loss: 1.1682\n",
      "Epoch 8/10, Batch 710/1465, Loss: 0.8318\n",
      "Epoch 8/10, Batch 720/1465, Loss: 0.7770\n",
      "Epoch 8/10, Batch 730/1465, Loss: 0.8020\n",
      "Epoch 8/10, Batch 740/1465, Loss: 0.8349\n",
      "Epoch 8/10, Batch 750/1465, Loss: 0.8097\n",
      "Epoch 8/10, Batch 760/1465, Loss: 0.7539\n",
      "Epoch 8/10, Batch 770/1465, Loss: 0.9967\n",
      "Epoch 8/10, Batch 780/1465, Loss: 0.9734\n",
      "Epoch 8/10, Batch 790/1465, Loss: 0.9806\n",
      "Epoch 8/10, Batch 800/1465, Loss: 1.0129\n",
      "Epoch 8/10, Batch 810/1465, Loss: 0.9103\n",
      "Epoch 8/10, Batch 820/1465, Loss: 0.7884\n",
      "Epoch 8/10, Batch 830/1465, Loss: 0.7893\n",
      "Epoch 8/10, Batch 840/1465, Loss: 1.0861\n",
      "Epoch 8/10, Batch 850/1465, Loss: 0.8714\n",
      "Epoch 8/10, Batch 860/1465, Loss: 0.7138\n",
      "Epoch 8/10, Batch 870/1465, Loss: 0.8748\n",
      "Epoch 8/10, Batch 880/1465, Loss: 0.8737\n",
      "Epoch 8/10, Batch 890/1465, Loss: 1.0816\n",
      "Epoch 8/10, Batch 900/1465, Loss: 0.7937\n",
      "Epoch 8/10, Batch 910/1465, Loss: 0.8136\n",
      "Epoch 8/10, Batch 920/1465, Loss: 0.6989\n",
      "Epoch 8/10, Batch 930/1465, Loss: 0.8450\n",
      "Epoch 8/10, Batch 940/1465, Loss: 1.1055\n",
      "Epoch 8/10, Batch 950/1465, Loss: 1.0048\n",
      "Epoch 8/10, Batch 960/1465, Loss: 0.9719\n",
      "Epoch 8/10, Batch 970/1465, Loss: 0.8522\n",
      "Epoch 8/10, Batch 980/1465, Loss: 1.0965\n",
      "Epoch 8/10, Batch 990/1465, Loss: 0.9829\n",
      "Epoch 8/10, Batch 1000/1465, Loss: 0.9072\n",
      "Epoch 8/10, Batch 1010/1465, Loss: 0.7918\n",
      "Epoch 8/10, Batch 1020/1465, Loss: 0.8278\n",
      "Epoch 8/10, Batch 1030/1465, Loss: 0.9745\n",
      "Epoch 8/10, Batch 1040/1465, Loss: 0.7910\n",
      "Epoch 8/10, Batch 1050/1465, Loss: 0.8093\n",
      "Epoch 8/10, Batch 1060/1465, Loss: 1.1473\n",
      "Epoch 8/10, Batch 1070/1465, Loss: 0.9082\n",
      "Epoch 8/10, Batch 1080/1465, Loss: 0.9494\n",
      "Epoch 8/10, Batch 1090/1465, Loss: 0.7634\n",
      "Epoch 8/10, Batch 1100/1465, Loss: 0.8166\n",
      "Epoch 8/10, Batch 1110/1465, Loss: 0.9208\n",
      "Epoch 8/10, Batch 1120/1465, Loss: 1.0027\n",
      "Epoch 8/10, Batch 1130/1465, Loss: 1.0350\n",
      "Epoch 8/10, Batch 1140/1465, Loss: 1.0206\n",
      "Epoch 8/10, Batch 1150/1465, Loss: 0.9656\n",
      "Epoch 8/10, Batch 1160/1465, Loss: 0.7749\n",
      "Epoch 8/10, Batch 1170/1465, Loss: 0.8508\n",
      "Epoch 8/10, Batch 1180/1465, Loss: 0.9943\n",
      "Epoch 8/10, Batch 1190/1465, Loss: 1.1146\n",
      "Epoch 8/10, Batch 1200/1465, Loss: 0.9085\n",
      "Epoch 8/10, Batch 1210/1465, Loss: 0.9967\n",
      "Epoch 8/10, Batch 1220/1465, Loss: 0.9165\n",
      "Epoch 8/10, Batch 1230/1465, Loss: 0.8668\n",
      "Epoch 8/10, Batch 1240/1465, Loss: 0.9354\n",
      "Epoch 8/10, Batch 1250/1465, Loss: 0.9388\n",
      "Epoch 8/10, Batch 1260/1465, Loss: 1.0536\n",
      "Epoch 8/10, Batch 1270/1465, Loss: 1.0000\n",
      "Epoch 8/10, Batch 1280/1465, Loss: 0.7385\n",
      "Epoch 8/10, Batch 1290/1465, Loss: 0.9145\n",
      "Epoch 8/10, Batch 1300/1465, Loss: 1.1397\n",
      "Epoch 8/10, Batch 1310/1465, Loss: 0.9677\n",
      "Epoch 8/10, Batch 1320/1465, Loss: 0.8963\n",
      "Epoch 8/10, Batch 1330/1465, Loss: 0.9120\n",
      "Epoch 8/10, Batch 1340/1465, Loss: 0.9745\n",
      "Epoch 8/10, Batch 1350/1465, Loss: 0.7037\n",
      "Epoch 8/10, Batch 1360/1465, Loss: 0.8687\n",
      "Epoch 8/10, Batch 1370/1465, Loss: 0.8309\n",
      "Epoch 8/10, Batch 1380/1465, Loss: 0.9629\n",
      "Epoch 8/10, Batch 1390/1465, Loss: 0.7695\n",
      "Epoch 8/10, Batch 1400/1465, Loss: 0.8577\n",
      "Epoch 8/10, Batch 1410/1465, Loss: 0.8926\n",
      "Epoch 8/10, Batch 1420/1465, Loss: 0.8570\n",
      "Epoch 8/10, Batch 1430/1465, Loss: 0.9964\n",
      "Epoch 8/10, Batch 1440/1465, Loss: 0.9256\n",
      "Epoch 8/10, Batch 1450/1465, Loss: 0.8974\n",
      "Epoch 8/10, Batch 1460/1465, Loss: 1.0987\n",
      "Epoch [8/10], Average Loss: 0.9055\n",
      "Epoch 9/10, Batch 10/1465, Loss: 0.9784\n",
      "Epoch 9/10, Batch 20/1465, Loss: 1.1446\n",
      "Epoch 9/10, Batch 30/1465, Loss: 0.9402\n",
      "Epoch 9/10, Batch 40/1465, Loss: 0.9409\n",
      "Epoch 9/10, Batch 50/1465, Loss: 0.8316\n",
      "Epoch 9/10, Batch 60/1465, Loss: 1.0039\n",
      "Epoch 9/10, Batch 70/1465, Loss: 0.9146\n",
      "Epoch 9/10, Batch 80/1465, Loss: 0.9088\n",
      "Epoch 9/10, Batch 90/1465, Loss: 0.9984\n",
      "Epoch 9/10, Batch 100/1465, Loss: 0.8584\n",
      "Epoch 9/10, Batch 110/1465, Loss: 0.8233\n",
      "Epoch 9/10, Batch 120/1465, Loss: 1.1577\n",
      "Epoch 9/10, Batch 130/1465, Loss: 0.7740\n",
      "Epoch 9/10, Batch 140/1465, Loss: 0.9401\n",
      "Epoch 9/10, Batch 150/1465, Loss: 0.7585\n",
      "Epoch 9/10, Batch 160/1465, Loss: 0.7258\n",
      "Epoch 9/10, Batch 170/1465, Loss: 0.8522\n",
      "Epoch 9/10, Batch 180/1465, Loss: 0.8203\n",
      "Epoch 9/10, Batch 190/1465, Loss: 0.7510\n",
      "Epoch 9/10, Batch 200/1465, Loss: 0.7339\n",
      "Epoch 9/10, Batch 210/1465, Loss: 0.8186\n",
      "Epoch 9/10, Batch 220/1465, Loss: 1.0678\n",
      "Epoch 9/10, Batch 230/1465, Loss: 1.1369\n",
      "Epoch 9/10, Batch 240/1465, Loss: 0.7407\n",
      "Epoch 9/10, Batch 250/1465, Loss: 0.7569\n",
      "Epoch 9/10, Batch 260/1465, Loss: 0.7228\n",
      "Epoch 9/10, Batch 270/1465, Loss: 0.8544\n",
      "Epoch 9/10, Batch 280/1465, Loss: 1.0021\n",
      "Epoch 9/10, Batch 290/1465, Loss: 0.7687\n",
      "Epoch 9/10, Batch 300/1465, Loss: 0.7235\n",
      "Epoch 9/10, Batch 310/1465, Loss: 0.8254\n",
      "Epoch 9/10, Batch 320/1465, Loss: 0.7126\n",
      "Epoch 9/10, Batch 330/1465, Loss: 0.9951\n",
      "Epoch 9/10, Batch 340/1465, Loss: 0.7513\n",
      "Epoch 9/10, Batch 350/1465, Loss: 0.8381\n",
      "Epoch 9/10, Batch 360/1465, Loss: 1.0669\n",
      "Epoch 9/10, Batch 370/1465, Loss: 0.9313\n",
      "Epoch 9/10, Batch 380/1465, Loss: 0.8561\n",
      "Epoch 9/10, Batch 390/1465, Loss: 0.7848\n",
      "Epoch 9/10, Batch 400/1465, Loss: 0.8446\n",
      "Epoch 9/10, Batch 410/1465, Loss: 0.7048\n",
      "Epoch 9/10, Batch 420/1465, Loss: 0.8801\n",
      "Epoch 9/10, Batch 430/1465, Loss: 0.9817\n",
      "Epoch 9/10, Batch 440/1465, Loss: 0.7784\n",
      "Epoch 9/10, Batch 450/1465, Loss: 1.0098\n",
      "Epoch 9/10, Batch 460/1465, Loss: 1.0102\n",
      "Epoch 9/10, Batch 470/1465, Loss: 0.8819\n",
      "Epoch 9/10, Batch 480/1465, Loss: 1.0175\n",
      "Epoch 9/10, Batch 490/1465, Loss: 0.9918\n",
      "Epoch 9/10, Batch 500/1465, Loss: 0.9416\n",
      "Epoch 9/10, Batch 510/1465, Loss: 0.8624\n",
      "Epoch 9/10, Batch 520/1465, Loss: 1.0823\n",
      "Epoch 9/10, Batch 530/1465, Loss: 0.9987\n",
      "Epoch 9/10, Batch 540/1465, Loss: 0.9359\n",
      "Epoch 9/10, Batch 550/1465, Loss: 0.8036\n",
      "Epoch 9/10, Batch 560/1465, Loss: 0.8997\n",
      "Epoch 9/10, Batch 570/1465, Loss: 1.0413\n",
      "Epoch 9/10, Batch 580/1465, Loss: 0.9354\n",
      "Epoch 9/10, Batch 590/1465, Loss: 0.9409\n",
      "Epoch 9/10, Batch 600/1465, Loss: 0.9146\n",
      "Epoch 9/10, Batch 610/1465, Loss: 0.7275\n",
      "Epoch 9/10, Batch 620/1465, Loss: 0.9446\n",
      "Epoch 9/10, Batch 630/1465, Loss: 0.8743\n",
      "Epoch 9/10, Batch 640/1465, Loss: 0.8944\n",
      "Epoch 9/10, Batch 650/1465, Loss: 0.7939\n",
      "Epoch 9/10, Batch 660/1465, Loss: 1.0727\n",
      "Epoch 9/10, Batch 670/1465, Loss: 0.9015\n",
      "Epoch 9/10, Batch 680/1465, Loss: 0.7555\n",
      "Epoch 9/10, Batch 690/1465, Loss: 1.0451\n",
      "Epoch 9/10, Batch 700/1465, Loss: 0.8691\n",
      "Epoch 9/10, Batch 710/1465, Loss: 0.9153\n",
      "Epoch 9/10, Batch 720/1465, Loss: 0.8439\n",
      "Epoch 9/10, Batch 730/1465, Loss: 0.7487\n",
      "Epoch 9/10, Batch 740/1465, Loss: 1.0200\n",
      "Epoch 9/10, Batch 750/1465, Loss: 0.7769\n",
      "Epoch 9/10, Batch 760/1465, Loss: 0.8093\n",
      "Epoch 9/10, Batch 770/1465, Loss: 0.9572\n",
      "Epoch 9/10, Batch 780/1465, Loss: 1.0613\n",
      "Epoch 9/10, Batch 790/1465, Loss: 0.9945\n",
      "Epoch 9/10, Batch 800/1465, Loss: 0.8682\n",
      "Epoch 9/10, Batch 810/1465, Loss: 0.8066\n",
      "Epoch 9/10, Batch 820/1465, Loss: 0.7522\n",
      "Epoch 9/10, Batch 830/1465, Loss: 0.8811\n",
      "Epoch 9/10, Batch 840/1465, Loss: 0.8170\n",
      "Epoch 9/10, Batch 850/1465, Loss: 0.7638\n",
      "Epoch 9/10, Batch 860/1465, Loss: 1.1240\n",
      "Epoch 9/10, Batch 870/1465, Loss: 0.7646\n",
      "Epoch 9/10, Batch 880/1465, Loss: 0.9759\n",
      "Epoch 9/10, Batch 890/1465, Loss: 0.9469\n",
      "Epoch 9/10, Batch 900/1465, Loss: 0.8054\n",
      "Epoch 9/10, Batch 910/1465, Loss: 0.9563\n",
      "Epoch 9/10, Batch 920/1465, Loss: 0.9734\n",
      "Epoch 9/10, Batch 930/1465, Loss: 0.9580\n",
      "Epoch 9/10, Batch 940/1465, Loss: 0.8737\n",
      "Epoch 9/10, Batch 950/1465, Loss: 0.9244\n",
      "Epoch 9/10, Batch 960/1465, Loss: 0.8813\n",
      "Epoch 9/10, Batch 970/1465, Loss: 0.8097\n",
      "Epoch 9/10, Batch 980/1465, Loss: 0.7565\n",
      "Epoch 9/10, Batch 990/1465, Loss: 1.0552\n",
      "Epoch 9/10, Batch 1000/1465, Loss: 0.9595\n",
      "Epoch 9/10, Batch 1010/1465, Loss: 0.6739\n",
      "Epoch 9/10, Batch 1020/1465, Loss: 0.9269\n",
      "Epoch 9/10, Batch 1030/1465, Loss: 0.8230\n",
      "Epoch 9/10, Batch 1040/1465, Loss: 0.7750\n",
      "Epoch 9/10, Batch 1050/1465, Loss: 1.1564\n",
      "Epoch 9/10, Batch 1060/1465, Loss: 0.8153\n",
      "Epoch 9/10, Batch 1070/1465, Loss: 0.9038\n",
      "Epoch 9/10, Batch 1080/1465, Loss: 0.8251\n",
      "Epoch 9/10, Batch 1090/1465, Loss: 0.7625\n",
      "Epoch 9/10, Batch 1100/1465, Loss: 0.8453\n",
      "Epoch 9/10, Batch 1110/1465, Loss: 1.1104\n",
      "Epoch 9/10, Batch 1120/1465, Loss: 0.8591\n",
      "Epoch 9/10, Batch 1130/1465, Loss: 0.9922\n",
      "Epoch 9/10, Batch 1140/1465, Loss: 1.0863\n",
      "Epoch 9/10, Batch 1150/1465, Loss: 0.9491\n",
      "Epoch 9/10, Batch 1160/1465, Loss: 0.9042\n",
      "Epoch 9/10, Batch 1170/1465, Loss: 1.1233\n",
      "Epoch 9/10, Batch 1180/1465, Loss: 0.8455\n",
      "Epoch 9/10, Batch 1190/1465, Loss: 0.7725\n",
      "Epoch 9/10, Batch 1200/1465, Loss: 1.0659\n",
      "Epoch 9/10, Batch 1210/1465, Loss: 0.8323\n",
      "Epoch 9/10, Batch 1220/1465, Loss: 0.8765\n",
      "Epoch 9/10, Batch 1230/1465, Loss: 0.7915\n",
      "Epoch 9/10, Batch 1240/1465, Loss: 0.7973\n",
      "Epoch 9/10, Batch 1250/1465, Loss: 1.1119\n",
      "Epoch 9/10, Batch 1260/1465, Loss: 1.0591\n",
      "Epoch 9/10, Batch 1270/1465, Loss: 1.0426\n",
      "Epoch 9/10, Batch 1280/1465, Loss: 1.0531\n",
      "Epoch 9/10, Batch 1290/1465, Loss: 0.8622\n",
      "Epoch 9/10, Batch 1300/1465, Loss: 0.9531\n",
      "Epoch 9/10, Batch 1310/1465, Loss: 1.0483\n",
      "Epoch 9/10, Batch 1320/1465, Loss: 0.9939\n",
      "Epoch 9/10, Batch 1330/1465, Loss: 1.0600\n",
      "Epoch 9/10, Batch 1340/1465, Loss: 0.9881\n",
      "Epoch 9/10, Batch 1350/1465, Loss: 0.9762\n",
      "Epoch 9/10, Batch 1360/1465, Loss: 1.0049\n",
      "Epoch 9/10, Batch 1370/1465, Loss: 0.7360\n",
      "Epoch 9/10, Batch 1380/1465, Loss: 0.7576\n",
      "Epoch 9/10, Batch 1390/1465, Loss: 1.0031\n",
      "Epoch 9/10, Batch 1400/1465, Loss: 0.9111\n",
      "Epoch 9/10, Batch 1410/1465, Loss: 0.9595\n",
      "Epoch 9/10, Batch 1420/1465, Loss: 1.0005\n",
      "Epoch 9/10, Batch 1430/1465, Loss: 1.2069\n",
      "Epoch 9/10, Batch 1440/1465, Loss: 0.7448\n",
      "Epoch 9/10, Batch 1450/1465, Loss: 0.8329\n",
      "Epoch 9/10, Batch 1460/1465, Loss: 0.8782\n",
      "Epoch [9/10], Average Loss: 0.8993\n",
      "Epoch 10/10, Batch 10/1465, Loss: 0.8766\n",
      "Epoch 10/10, Batch 20/1465, Loss: 0.8963\n",
      "Epoch 10/10, Batch 30/1465, Loss: 1.0119\n",
      "Epoch 10/10, Batch 40/1465, Loss: 0.8004\n",
      "Epoch 10/10, Batch 50/1465, Loss: 0.7850\n",
      "Epoch 10/10, Batch 60/1465, Loss: 0.8448\n",
      "Epoch 10/10, Batch 70/1465, Loss: 0.7978\n",
      "Epoch 10/10, Batch 80/1465, Loss: 0.8020\n",
      "Epoch 10/10, Batch 90/1465, Loss: 0.8480\n",
      "Epoch 10/10, Batch 100/1465, Loss: 1.0818\n",
      "Epoch 10/10, Batch 110/1465, Loss: 0.8603\n",
      "Epoch 10/10, Batch 120/1465, Loss: 0.9399\n",
      "Epoch 10/10, Batch 130/1465, Loss: 0.9947\n",
      "Epoch 10/10, Batch 140/1465, Loss: 0.9627\n",
      "Epoch 10/10, Batch 150/1465, Loss: 1.0364\n",
      "Epoch 10/10, Batch 160/1465, Loss: 0.9583\n",
      "Epoch 10/10, Batch 170/1465, Loss: 0.9568\n",
      "Epoch 10/10, Batch 180/1465, Loss: 0.8542\n",
      "Epoch 10/10, Batch 190/1465, Loss: 0.8875\n",
      "Epoch 10/10, Batch 200/1465, Loss: 0.9283\n",
      "Epoch 10/10, Batch 210/1465, Loss: 0.9151\n",
      "Epoch 10/10, Batch 220/1465, Loss: 0.9348\n",
      "Epoch 10/10, Batch 230/1465, Loss: 0.8585\n",
      "Epoch 10/10, Batch 240/1465, Loss: 0.8798\n",
      "Epoch 10/10, Batch 250/1465, Loss: 1.0298\n",
      "Epoch 10/10, Batch 260/1465, Loss: 0.8997\n",
      "Epoch 10/10, Batch 270/1465, Loss: 0.8383\n",
      "Epoch 10/10, Batch 280/1465, Loss: 1.0943\n",
      "Epoch 10/10, Batch 290/1465, Loss: 0.8284\n",
      "Epoch 10/10, Batch 300/1465, Loss: 0.9228\n",
      "Epoch 10/10, Batch 310/1465, Loss: 0.9614\n",
      "Epoch 10/10, Batch 320/1465, Loss: 0.8557\n",
      "Epoch 10/10, Batch 330/1465, Loss: 0.9122\n",
      "Epoch 10/10, Batch 340/1465, Loss: 0.9334\n",
      "Epoch 10/10, Batch 350/1465, Loss: 0.7916\n",
      "Epoch 10/10, Batch 360/1465, Loss: 1.1189\n",
      "Epoch 10/10, Batch 370/1465, Loss: 1.0584\n",
      "Epoch 10/10, Batch 380/1465, Loss: 0.9363\n",
      "Epoch 10/10, Batch 390/1465, Loss: 0.9475\n",
      "Epoch 10/10, Batch 400/1465, Loss: 0.9971\n",
      "Epoch 10/10, Batch 410/1465, Loss: 0.9151\n",
      "Epoch 10/10, Batch 420/1465, Loss: 0.8317\n",
      "Epoch 10/10, Batch 430/1465, Loss: 1.1511\n",
      "Epoch 10/10, Batch 440/1465, Loss: 0.9113\n",
      "Epoch 10/10, Batch 450/1465, Loss: 0.8796\n",
      "Epoch 10/10, Batch 460/1465, Loss: 0.8622\n",
      "Epoch 10/10, Batch 470/1465, Loss: 0.7228\n",
      "Epoch 10/10, Batch 480/1465, Loss: 0.8107\n",
      "Epoch 10/10, Batch 490/1465, Loss: 0.9060\n",
      "Epoch 10/10, Batch 500/1465, Loss: 1.0085\n",
      "Epoch 10/10, Batch 510/1465, Loss: 1.0188\n",
      "Epoch 10/10, Batch 520/1465, Loss: 0.7023\n",
      "Epoch 10/10, Batch 530/1465, Loss: 0.5827\n",
      "Epoch 10/10, Batch 540/1465, Loss: 0.8982\n",
      "Epoch 10/10, Batch 550/1465, Loss: 0.8127\n",
      "Epoch 10/10, Batch 560/1465, Loss: 0.8064\n",
      "Epoch 10/10, Batch 570/1465, Loss: 0.7795\n",
      "Epoch 10/10, Batch 580/1465, Loss: 0.8387\n",
      "Epoch 10/10, Batch 590/1465, Loss: 0.8471\n",
      "Epoch 10/10, Batch 600/1465, Loss: 1.0176\n",
      "Epoch 10/10, Batch 610/1465, Loss: 0.8217\n",
      "Epoch 10/10, Batch 620/1465, Loss: 0.9280\n",
      "Epoch 10/10, Batch 630/1465, Loss: 0.8309\n",
      "Epoch 10/10, Batch 640/1465, Loss: 0.7885\n",
      "Epoch 10/10, Batch 650/1465, Loss: 0.7250\n",
      "Epoch 10/10, Batch 660/1465, Loss: 0.7938\n",
      "Epoch 10/10, Batch 670/1465, Loss: 0.9291\n",
      "Epoch 10/10, Batch 680/1465, Loss: 1.0661\n",
      "Epoch 10/10, Batch 690/1465, Loss: 1.0679\n",
      "Epoch 10/10, Batch 700/1465, Loss: 0.9823\n",
      "Epoch 10/10, Batch 710/1465, Loss: 0.9050\n",
      "Epoch 10/10, Batch 720/1465, Loss: 0.8199\n",
      "Epoch 10/10, Batch 730/1465, Loss: 0.8960\n",
      "Epoch 10/10, Batch 740/1465, Loss: 1.0959\n",
      "Epoch 10/10, Batch 750/1465, Loss: 0.8619\n",
      "Epoch 10/10, Batch 760/1465, Loss: 1.0096\n",
      "Epoch 10/10, Batch 770/1465, Loss: 0.9306\n",
      "Epoch 10/10, Batch 780/1465, Loss: 0.8847\n",
      "Epoch 10/10, Batch 790/1465, Loss: 1.0062\n",
      "Epoch 10/10, Batch 800/1465, Loss: 0.8526\n",
      "Epoch 10/10, Batch 810/1465, Loss: 0.9330\n",
      "Epoch 10/10, Batch 820/1465, Loss: 0.8376\n",
      "Epoch 10/10, Batch 830/1465, Loss: 1.0125\n",
      "Epoch 10/10, Batch 840/1465, Loss: 0.7854\n",
      "Epoch 10/10, Batch 850/1465, Loss: 0.8411\n",
      "Epoch 10/10, Batch 860/1465, Loss: 0.9131\n",
      "Epoch 10/10, Batch 870/1465, Loss: 0.7865\n",
      "Epoch 10/10, Batch 880/1465, Loss: 0.8092\n",
      "Epoch 10/10, Batch 890/1465, Loss: 0.9821\n",
      "Epoch 10/10, Batch 900/1465, Loss: 0.7733\n",
      "Epoch 10/10, Batch 910/1465, Loss: 0.8066\n",
      "Epoch 10/10, Batch 920/1465, Loss: 0.8533\n",
      "Epoch 10/10, Batch 930/1465, Loss: 1.1144\n",
      "Epoch 10/10, Batch 940/1465, Loss: 1.1160\n",
      "Epoch 10/10, Batch 950/1465, Loss: 0.8066\n",
      "Epoch 10/10, Batch 960/1465, Loss: 0.9009\n",
      "Epoch 10/10, Batch 970/1465, Loss: 1.0221\n",
      "Epoch 10/10, Batch 980/1465, Loss: 0.9065\n",
      "Epoch 10/10, Batch 990/1465, Loss: 1.0175\n",
      "Epoch 10/10, Batch 1000/1465, Loss: 0.8325\n",
      "Epoch 10/10, Batch 1010/1465, Loss: 0.7318\n",
      "Epoch 10/10, Batch 1020/1465, Loss: 0.7946\n",
      "Epoch 10/10, Batch 1030/1465, Loss: 1.0320\n",
      "Epoch 10/10, Batch 1040/1465, Loss: 0.8485\n",
      "Epoch 10/10, Batch 1050/1465, Loss: 0.7821\n",
      "Epoch 10/10, Batch 1060/1465, Loss: 1.0060\n",
      "Epoch 10/10, Batch 1070/1465, Loss: 0.7145\n",
      "Epoch 10/10, Batch 1080/1465, Loss: 0.8456\n",
      "Epoch 10/10, Batch 1090/1465, Loss: 0.8440\n",
      "Epoch 10/10, Batch 1100/1465, Loss: 0.7585\n",
      "Epoch 10/10, Batch 1110/1465, Loss: 0.8323\n",
      "Epoch 10/10, Batch 1120/1465, Loss: 0.9916\n",
      "Epoch 10/10, Batch 1130/1465, Loss: 0.8964\n",
      "Epoch 10/10, Batch 1140/1465, Loss: 0.7117\n",
      "Epoch 10/10, Batch 1150/1465, Loss: 0.9899\n",
      "Epoch 10/10, Batch 1160/1465, Loss: 0.8244\n",
      "Epoch 10/10, Batch 1170/1465, Loss: 0.7199\n",
      "Epoch 10/10, Batch 1180/1465, Loss: 0.7608\n",
      "Epoch 10/10, Batch 1190/1465, Loss: 0.9915\n",
      "Epoch 10/10, Batch 1200/1465, Loss: 1.0085\n",
      "Epoch 10/10, Batch 1210/1465, Loss: 1.1166\n",
      "Epoch 10/10, Batch 1220/1465, Loss: 0.8903\n",
      "Epoch 10/10, Batch 1230/1465, Loss: 0.9420\n",
      "Epoch 10/10, Batch 1240/1465, Loss: 0.8248\n",
      "Epoch 10/10, Batch 1250/1465, Loss: 1.0777\n",
      "Epoch 10/10, Batch 1260/1465, Loss: 1.2017\n",
      "Epoch 10/10, Batch 1270/1465, Loss: 0.9929\n",
      "Epoch 10/10, Batch 1280/1465, Loss: 1.0095\n",
      "Epoch 10/10, Batch 1290/1465, Loss: 0.9207\n",
      "Epoch 10/10, Batch 1300/1465, Loss: 0.6775\n",
      "Epoch 10/10, Batch 1310/1465, Loss: 0.8307\n",
      "Epoch 10/10, Batch 1320/1465, Loss: 0.8547\n",
      "Epoch 10/10, Batch 1330/1465, Loss: 0.7854\n",
      "Epoch 10/10, Batch 1340/1465, Loss: 1.0403\n",
      "Epoch 10/10, Batch 1350/1465, Loss: 0.7762\n",
      "Epoch 10/10, Batch 1360/1465, Loss: 0.8361\n",
      "Epoch 10/10, Batch 1370/1465, Loss: 0.8340\n",
      "Epoch 10/10, Batch 1380/1465, Loss: 0.8902\n",
      "Epoch 10/10, Batch 1390/1465, Loss: 0.8727\n",
      "Epoch 10/10, Batch 1400/1465, Loss: 0.7961\n",
      "Epoch 10/10, Batch 1410/1465, Loss: 0.8253\n",
      "Epoch 10/10, Batch 1420/1465, Loss: 1.0503\n",
      "Epoch 10/10, Batch 1430/1465, Loss: 1.1282\n",
      "Epoch 10/10, Batch 1440/1465, Loss: 0.8538\n",
      "Epoch 10/10, Batch 1450/1465, Loss: 0.7876\n",
      "Epoch 10/10, Batch 1460/1465, Loss: 1.0016\n",
      "Epoch [10/10], Average Loss: 0.8928\n",
      "Finished Training\n",
      "Accuracy: 63.72561574251932%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"netflix_reviews.csv\")  # 파일 불러오기\n",
    "df = df.iloc[:,0:5]\n",
    "\n",
    "# 전처리 함수\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "\n",
    "# 이모티콘만 추출하는 함수 (중복 제거)\n",
    "def remove_duplicate_emojis(text):\n",
    "    # 유니코드 이모티콘 범위에 해당하는 모든 이모티콘을 찾음\n",
    "    emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F]\", flags=re.UNICODE)\n",
    "    \n",
    "    # 중복 제거를 위한 세트 (set) 사용\n",
    "    emojis = set(emoji_pattern.findall(text))\n",
    "    \n",
    "    # 텍스트에서 중복된 이모티콘을 제거하고, 하나의 이모티콘만 남김\n",
    "    for em in emojis:\n",
    "        text = re.sub(em + '+', em, text)  # 중복된 이모티콘을 하나로 줄임\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 전처리 함수 (이모티콘 중복 제거 후 텍스트로 변환)\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, float):\n",
    "        return \"\"\n",
    "    \n",
    "    # 이모티콘 중복 제거\n",
    "    text = remove_duplicate_emojis(text)\n",
    "    \n",
    "    # 이모티콘을 텍스트로 변환\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    \n",
    "    # 소문자로 변환\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 숫자 및 구두점 제거\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # 앞뒤 공백 제거\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "df['reviewId'] = df['reviewId'].apply(preprocess_text)\n",
    "df['userName'] = df['userName'].apply(preprocess_text)\n",
    "df['content'] = df['content'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "#\n",
    "reviews = df['content'].tolist()  # 'content'를 리스트로 변환\n",
    "ratings = df['score'].tolist()    # 'score'를 리스트로 변환\n",
    "\n",
    "\n",
    "\n",
    "# 라벨을 정수형으로 변환 (필수적인 과정)\n",
    "label_encoder = LabelEncoder()\n",
    "ratings = label_encoder.fit_transform(ratings)  # 평점 정수형으로 변환\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, ratings, text_pipeline, label_pipeline):\n",
    "        self.reviews = reviews\n",
    "        self.ratings = ratings\n",
    "        self.text_pipeline = text_pipeline\n",
    "        self.label_pipeline = label_pipeline\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.text_pipeline(self.reviews[idx])\n",
    "        rating = self.label_pipeline(self.ratings[idx])\n",
    "        return torch.tensor(review), torch.tensor(rating)\n",
    "\n",
    "# 토크나이저 정의 (기본 영어 토크나이저)\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# 어휘 사전 생성 함수\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# 어휘 사전 생성\n",
    "vocab = build_vocab_from_iterator(yield_tokens(reviews))\n",
    "\n",
    "\n",
    "# 텍스트 파이프라인 정의 (어휘 사전에 있는 단어만 처리)\n",
    "def text_pipeline(text):\n",
    "    return [vocab[token] for token in tokenizer(text)]\n",
    "\n",
    "# 평점 그대로 사용\n",
    "def label_pipeline(label):\n",
    "    return label  # 이미 숫자형이므로 변환 생략\n",
    "\n",
    "# 데이터를 학습용(train)과 테스트용(test)으로 분리\n",
    "train_reviews, test_reviews, train_ratings, test_ratings = train_test_split(reviews, ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# 데이터셋 정의\n",
    "train_dataset = ReviewDataset(train_reviews, train_ratings, text_pipeline, label_pipeline)\n",
    "test_dataset = ReviewDataset(test_reviews, test_ratings, text_pipeline, label_pipeline)\n",
    "\n",
    "# 패딩을 적용하는 함수 정의\n",
    "\n",
    "def collate_fn(batch):\n",
    "    reviews, ratings = zip(*batch)\n",
    "    reviews = pad_sequence([torch.tensor(r, dtype=torch.long) for r in reviews], batch_first=True)  # 정수형 텐서로 변환\n",
    "    ratings = torch.tensor(ratings, dtype=torch.long)  # 평점도 정수형으로 변환\n",
    "    return reviews, ratings\n",
    "# 데이터 로더 정의\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# LSTM 모델 정의\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # Embedding으로 변경\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        return self.fc(hidden[-1])\n",
    "\n",
    "# 하이퍼파라미터 정의\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBED_DIM = 64\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = len(set(ratings))  # 예측할 점수 개수 (평점이 정수형)\n",
    "\n",
    "# 모델 초기화\n",
    "model = LSTMModel(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # SGD 에서 Adam으로 변경 lr : 0.01 - > 0.001 / Accuracy: 63% -> 61.59% 다시 \n",
    "\n",
    "# 모델을 CUDA로 이동 (가능한 경우)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 모델 학습 함수 정의\n",
    "def train_model(model, train_dataloader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()  # 학습 모드로 설정\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0  # 에포크마다 손실을 추적\n",
    "        for i, (reviews, ratings) in enumerate(train_dataloader):\n",
    "            reviews, ratings = reviews.to(device), ratings.to(device)  # 데이터를 GPU로 이동\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(reviews)  # 모델에 입력하여 예측값 계산\n",
    "            loss = criterion(outputs, ratings)  # 손실 계산\n",
    "            loss.backward()  # 역전파\n",
    "            optimizer.step()  # 가중치 업데이트\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 배치마다 손실 출력\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_dataloader)}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {total_loss/len(train_dataloader):.4f}')\n",
    "    \n",
    "    print(\"Finished Training\")\n",
    "\n",
    "# 모델 학습 실행\n",
    "train_model(model, train_dataloader, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "# 모델 평가\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # 평가 시에는 기울기 계산을 하지 않음\n",
    "    for reviews, ratings in test_dataloader:\n",
    "        reviews, ratings = reviews.to(device), ratings.to(device)\n",
    "        outputs = model(reviews)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += ratings.size(0)\n",
    "        correct += (predicted == ratings).sum().item()\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c329d1eb-efaa-4818-9597-4ee0919af35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Score: 5\n"
     ]
    }
   ],
   "source": [
    "# 예측 함수\n",
    "def predict_review(model, review, vocab, tokenizer, device):\n",
    "    # 리뷰를 텐서로 변환\n",
    "    tokens = [vocab[token] for token in tokenizer(review)]\n",
    "    review_tensor = torch.tensor(tokens).unsqueeze(0)  # (1, seq_length) 형태로 만듦\n",
    "    \n",
    "    # 텐서를 GPU로 이동\n",
    "    review_tensor = review_tensor.to(device)\n",
    "    \n",
    "    # 모델에 입력하여 예측값 계산\n",
    "    model.eval()  # 평가 모드로 변경\n",
    "    with torch.no_grad():  # 평가 시에는 기울기 계산을 하지 않음\n",
    "        output = model(review_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "    \n",
    "    return predicted.item() + 1 # 예측된 평점 반환 0~4값을 1~5로 바꾸기 위해서 +1\n",
    "# 새로운 리뷰에 대한 예측\n",
    "new_review = \"This app is great but has some bugs.\"\n",
    "predicted_score = predict_review(model, new_review, vocab, tokenizer,  device)\n",
    "print(f'Predicted Score: {predicted_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f1a5803-f49b-4a9c-ae74-56cbc8551655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Score: 3\n"
     ]
    }
   ],
   "source": [
    "# 새로운 리뷰에 대한 예측2\n",
    "new_review = \"Good app for streaming occasionally, but this is the only app I have that completely malfunctions my user interface, and forces me to restart my phone. No idea why it happens, but it is profoundly annoying.\"\n",
    "predicted_score = predict_review(model, new_review, vocab, tokenizer,  device)\n",
    "print(f'Predicted Score: {predicted_score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
