### 딥러닝에서 선형대수학을 사용하는 이유

딥러닝과 같은 AI 시스템에서 **선형대수학**은 데이터를 효율적으로 표현하고 처리하는 데 필수적인 도구입니다. 수많은 매개변수와 데이터 포인트를 다루면서 벡터와 행렬을 사용해 연산을 수행하고 패턴을 학습하는 것이 주된 이유입니다.

---

### 1. 선형대수학이란?

선형대수학은 **벡터**와 **행렬** 등의 수학적 개념을 다루는 학문입니다. 딥러닝의 기본 요소인 신경망에서는 이러한 개념들이 데이터의 관계를 표현하고 학습하는 데 매우 중요한 역할을 합니다. 예를 들어, 이미지 데이터를 숫자로 변환하여 연산하는 과정에서 선형대수학이 핵심적인 역할을 합니다.

---

### 2. 벡터와 행렬의 기본 개념

- **벡터**: 크기와 방향을 가진 값들의 모음으로, 데이터를 하나의 열 혹은 행으로 표현할 수 있는 구조입니다.
- **행렬**: 여러 벡터를 모아서 만든 2차원 배열로, 데이터의 많은 특성을 다룰 때 유용합니다. 예를 들어, 하나의 이미지는 픽셀 값들을 벡터로 표현할 수 있고, 여러 이미지들을 모아 하나의 행렬로 표현할 수 있습니다.

---

### 3. 행렬의 개념

행렬은 여러 벡터를 하나의 구조로 모은 것입니다. 이는 딥러닝에서 각 입력 데이터를 여러 특성(feature)들로 정리해 쉽게 다루기 위한 방법입니다. 예를 들어, 데이터셋을 하나의 행렬로 표현하면 각 행이 개별 데이터 포인트를 나타내고, 각 열이 데이터의 특성을 나타냅니다.

---

### 4. 벡터의 개념

벡터는 데이터 포인트들을 한 줄로 나타낸 구조입니다. 예를 들어, **RGB 이미지**를 표현할 때 각 픽셀의 RGB 값들을 하나의 벡터로 나타낼 수 있습니다. 벡터는 주로 특성 공간(feature space)에서의 위치를 나타내는 데 사용됩니다.

---

### 5. 벡터의 연산

벡터의 연산은 딥러닝에서 중요한 기초입니다. 벡터의 **합, 차, 스칼라 곱** 등을 통해 데이터를 결합하거나 변환할 수 있습니다. 예를 들어, 이미지 처리에서 두 벡터를 더해 새로운 이미지를 만들거나, 특정 값을 곱해 밝기를 조정할 수 있습니다.

---

### 6. 벡터의 크기

벡터의 **크기(또는 길이)**는 벡터의 중요성을 수량화하는 데 사용됩니다. 벡터의 크기는 데이터 간의 거리를 측정하거나, 어떤 데이터 포인트가 얼마나 중요한지 판단하는 데 사용됩니다. 예를 들어, 두 사용자 간의 영화 취향의 유사도를 비교할 때 벡터의 크기를 사용하여 거리를 계산할 수 있습니다.

---

### 7. 벡터의 내적 (딥러닝 학습 중심)

벡터의 **내적**은 두 벡터 사이의 관계를 수량적으로 표현하는 연산입니다. 딥러닝에서는 주로 뉴런 간의 **가중합(weighted sum)**을 계산하기 위해 사용됩니다. 내적은 입력 벡터와 가중치 벡터의 곱을 합산하는 방식으로 진행됩니다.

$$
\mathbf{w} = [0.2, 0.8, -0.5], \quad \mathbf{x} = [1, 0.5, 2]
$$

$$
0.2 \times 1 + 0.8 \times 0.5 + (-0.5) \times 2 = 0.2 + 0.4 - 1 = -0.4
$$

이 결과는 해당 뉴런의 출력값으로 사용될 수 있으며, 이는 신호의 강도를 의미합니다. 값이 클수록 해당 입력이 더 큰 영향을 미친다는 것을 나타냅니다.

---

### 8. 코사인 유사도 (벡터 간 유사성 측정)

**코사인 유사도**는 벡터 간의 각도로 유사성을 측정하는 방법입니다. 이는 두 벡터가 얼마나 비슷한 방향을 가지는지 나타내며, 다음과 같은 수식으로 계산됩니다:

$$
\text{cosine similarity} = \frac{ \mathbf{A} \cdot \mathbf{B} }{ \| \mathbf{A} \| \| \mathbf{B} \| }
$$

여기서 $( \mathbf{A} )$와 $( \mathbf{B} )$는 두 벡터이고, $( \| \mathbf{A} \| )$와 $( \| \mathbf{B} \| )$는 각 벡터의 크기(또는 노름)입니다. 

- 값이 **1**에 가까울수록 두 벡터는 동일한 방향을 가지며,
- 값이 **0**일 경우 두 벡터는 수직,
- 값이 **-1**에 가까울수록 반대 방향을 가집니다.

예를 들어, 두 개의 문서 벡터가 각각 $( \mathbf{A} = [1, 0, 1, 1] )$, $( \mathbf{B} = [1, 1, 0, 1] )$인 경우, 코사인 유사도는 다음과 같이 계산됩니다:

- 내적: $( 1 \times 1 + 0 \times 1 + 1 \times 0 + 1 \times 1 = 2 )$
- 벡터 A의 크기: $( \sqrt{1^2 + 0^2 + 1^2 + 1^2} = \sqrt{3} )$
- 벡터 B의 크기: $( \sqrt{1^2 + 1^2 + 0^2 + 1^2} = \sqrt{3} )$
- 코사인 유사도: $( \frac{2}{\sqrt{3} \times \sqrt{3}} = \frac{2}{3} \approx 0.67 )$

이 값은 두 벡터가 유사하지만 동일하지는 않다는 것을 의미합니다. 코사인 유사도는 문서 검색, 추천 시스템 등에서 두 데이터 간의 유사성을 수치화할 때 매우 유용하게 사용됩니다.

---

### 9. 벡터의 연산의 활용 (Word Embedding)

**Word2Vec** 같은 word embedding 기법에서 벡터 연산을 통해 단어 간 의미적인 연관성을 파악할 수 있습니다. 예를 들어, "king - man + woman = queen"과 같은 연산이 가능할 정도로 벡터 공간에 의미가 내재화됩니다. 이는 단어 간의 관계를 수학적으로 표현할 수 있게 해 주며, 의미적 유사성을 기반으로 새로운 관계를 유추할 수 있습니다.

---

### 10. 코사인 유사도의 활용

- **문서 유사도 측정**: 문서 간의 유사도를 비교하여 검색 결과를 개선할 수 있습니다. 예를 들어, 특정 키워드와 가장 유사한 문서를 찾을 수 있습니다.
- **이미지 검색**: 이미지의 특징 벡터 간 유사도를 계산해 유사한 이미지를 찾아냅니다. 예를 들어, 사용자가 업로드한 이미지와 유사한 이미지를 데이터베이스에서 검색합니다.
- **추천 시스템**: 사용자가 선호하는 아이템과 유사한 특성을 가진 아이템을 추천하는 데 사용됩니다. 예를 들어, 사용자가 좋아하는 영화와 비슷한 특성을 가진 다른 영화를 추천할 수 있습니다.

---

